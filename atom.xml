<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sherlockの小酒馆</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.sherlocknlp.com/"/>
  <updated>2019-07-03T13:57:01.940Z</updated>
  <id>http://www.sherlocknlp.com/</id>
  
  <author>
    <name>Sherlock</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>小冰架构学习</title>
    <link href="http://www.sherlocknlp.com/%E5%B0%8F%E5%86%B0%E6%9E%B6%E6%9E%84%E5%AD%A6%E4%B9%A0/"/>
    <id>http://www.sherlocknlp.com/小冰架构学习/</id>
    <published>2019-07-03T11:30:07.631Z</published>
    <updated>2019-07-03T13:57:01.940Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.nlark.com/yuque/0/2019/png/104214/1546396005189-1beef810-d825-4ea8-8597-3806892e92d4.png" alt></p><h2 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h2><p><strong>如何评价小冰Open Domain Social Long-term Dialogue System的性能？</strong></p><p>单次对话论述 Conversation-turns Per Session(CPS)作为评估社交聊天机器人成功的指标。CPS越大，社交聊天机器人的对话参与能力就越好。第六代小冰的CPS已经从第一代的5提升到23。</p><p><strong>IQ+EQ+Personality</strong></p><p>社交聊天机器人需要足够高的智商（IQ）来学习多种技能，才能紧跟用户需求，帮助他们完成指定任务。同时，社交聊天机器人还需要足够高的情商（EQ），以满足用户情感的需求，比如情绪感受和社会归属感。</p><ul><li><strong>IQ：</strong>IQ能力包括知识和记忆建模、图像和自然语言理解、推理生成和预测。为了满足用户的特定需求以及帮助用户完成指定的任务，这些能力是不可或缺的。其中最重要且最复杂的技能是核心聊天（Core Chat），即与用户在多个主题上展开长时间和开放域的对话</li><li><strong>EQ：</strong></li><li><strong>Personality：</strong>符合小冰既有的风格</li></ul><p><strong>将社交聊天视为分层决策</strong></p><p>对话可被视为有自然层级的决策过程：一个顶级过程管理者整体的对话并选取不同的技能来处理不同类型 的对话模式（比如闲聊、问答、订票）；低级过程则受所选择的技能控制。可选择基本动作（响应），从而生成对话段落或完成任务。</p><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p><img src="https://cdn.nlark.com/yuque/0/2019/png/104214/1546398672251-b8e722bc-4c4f-4a68-b363-c5f85ec5269a.png" alt></p><p>上图为小冰的整体架构，其包含三层：用户体验层、对话引擎层和数据层。</p><h3 id="用户体验层"><a href="#用户体验层" class="headerlink" title="用户体验层"></a>用户体验层</h3><h3 id="对话引擎层"><a href="#对话引擎层" class="headerlink" title="对话引擎层"></a>对话引擎层</h3><p>对话引擎层中的四个主要组件：对话管理、共情计算、核心聊天和技能。</p><h4 id="对话管理"><a href="#对话管理" class="headerlink" title="对话管理"></a>对话管理</h4><p>对话管理除了记录<strong>对话历史</strong>还要包括策略管理，即管理什么时候触发Skill什么时候切换。同时，Conversation还受到Topic的管理。在Pretrain的阶段得到Topic Index，当触发Topic切换的标志时，例如：</p><ol><li>Core Chat未能生成有效的候选集</li><li>生成的响应知识用户输入的重复</li><li>用户输入变得平淡</li></ol><p>则调用Topic切换，切换之后的Topic根据以下几个指标选取：</p><ol><li>上下文关联性</li><li>新鲜度</li><li>个人兴趣</li><li>热度</li><li>接受度</li></ol><h4 id="共情计算（Empathetic-Computing）"><a href="#共情计算（Empathetic-Computing）" class="headerlink" title="共情计算（Empathetic Computing）"></a>共情计算（Empathetic Computing）</h4><p>共情度计算是小冰相对独特的一点，它不是直接把Context和Reply进行匹配得到一个Match Score。而是由Query, Context, Reply及情景分析得到一个$s=\left(Q_{c}, C, e_{Q}, e_{R}\right)$向量，再由向量$s$计算出候选回复。</p><p><strong>Contextual Query Understanding</strong></p><p>本模块的主要任务是对Context进行补全，通过命名实体识别和指代消解的方法，对对话历史信息进行补全。</p><p><strong>User Understand</strong></p><p><strong>Interpersonal Response Generation</strong></p><h4 id="核心聊天（Core-chat）"><a href="#核心聊天（Core-chat）" class="headerlink" title="核心聊天（Core chat）"></a>核心聊天（Core chat）</h4><p>小冰在处理语言生成的时候使用两阶段法，先构造Reply候选集，然后通过计算每个Reply的得分（Score）选出最优答案。</p><p><img src="http://image.sherlocknlp.com/image/20190627/LYBy1QUaoGK4.png" alt="mark"></p><p><strong>Retrieval-Based Using Paired Data</strong></p><p><strong>Neural Generate</strong></p><p>单纯靠检索来获得数据，会漏掉一些新的热点，覆盖面不高。在小冰的架构中，对前面构造的向量$s$做sigmoid操作$v=\sigma\left(W_{Q}^{T} e_{Q}+W_{R}^{T} e_{R}\right)$，每一轮都喂到网络中这样保证了生成的Response带有小冰的<code>人格</code>。</p><p><strong>Retrieval-Based Using Unpaired Data</strong></p><p><strong>Rank</strong></p><h3 id="数据层"><a href="#数据层" class="headerlink" title="数据层"></a>数据层</h3><p><img src="http://image.sherlocknlp.com/image/20190627/ssSoBWfXdGoo.png" alt="mark"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://wyydsb.xin/" target="_blank" rel="noopener">直男届的杀手-『小冰』架构解析</a></p><p><a href="https://zhuanlan.zhihu.com/p/53667904" target="_blank" rel="noopener">沈向洋等人论文详解微软小冰，公开研发细节</a></p><p><a href="https://arxiv.org/pdf/1812.08989.pdf" target="_blank" rel="noopener">The Design and Implementation of XiaoIce, an Empathetic Social Chatbot. Li Zhou et al. 18.12</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2019/png/104214/1546396005189-1beef810-d825-4ea8-8597-3806892e92d4.png&quot; alt&gt;&lt;/p&gt;
&lt;h2 id=&quot;设计思想&quot;&gt;&lt;a
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《通过对响应原型编辑的回复生成》阅读笔记</title>
    <link href="http://www.sherlocknlp.com/%E3%80%8A%E9%80%9A%E8%BF%87%E5%AF%B9%E5%93%8D%E5%BA%94%E5%8E%9F%E5%9E%8B%E7%BC%96%E8%BE%91%E7%9A%84%E5%9B%9E%E5%A4%8D%E7%94%9F%E6%88%90%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.sherlocknlp.com/《通过对响应原型编辑的回复生成》阅读笔记/</id>
    <published>2019-07-03T11:29:46.590Z</published>
    <updated>2019-07-03T08:44:53.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ps1hmyafs.bkt.clouddn.com/image/20190531/JBLOFiIj98jN.png" alt="mark"></p><p>论文题目：<a href="https://arxiv.org/abs/1806.07042" target="_blank" rel="noopener">Response Generation by Context-aware Prototype Editing</a></p><p>代码链接：<a href="https://github.com/MarkWuNLP/ResponseEdit" target="_blank" rel="noopener">https://github.com/MarkWuNLP/ResponseEdit</a></p><p>AAAI 2019 北航</p><p>本文的<strong>主要目的</strong>是为了解决开放域对话生成中，生成句子较短且无意义的问题。模型首先从预定义的索引中检索到响应，然后根据问题原型和当前问题的差异编辑响应原型。主要思想是基于<strong>由于响应原型有很好的语法和信息，对响应原型进行轻微地调整就能得到较好的回复</strong>这一假设，来进行相关实验和论证。这是一种<strong>“检索+生成”</strong>的方法，既有检索式聊天机器人回复流畅和信息量大的优势，还有生成式聊天机器人的灵活性和相关性。</p><p><img src="http://ps1hmyafs.bkt.clouddn.com/FvX6HP7sPZxLKZp34lUpIRRdFkuq" alt></p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>该模型由原型选择器$\mathcal{S}$和上下文感知编辑器$\mathcal{E}$组成。给定一个新的会话，我们首先使用选择器S来检索语料$\mathcal{D} = {(C_i, R_i)}_{i=1}^{N}$中的$(C_{i}, R_{i}) \in \mathcal{D}$。然后编辑器$\mathcal{E}$计算出$z_i = f(C_i, C)$来编码$C_i$和$C$之间的差异信息。最后，通过概率$p(R|z_i, R_i)$生成回复。</p><p><img src="http://ps1hmyafs.bkt.clouddn.com/FjSHtNN5ZpLNsSmzX2LfHhPyJfUO" alt></p><h3 id="原型选择器-Prototype-Selector"><a href="#原型选择器-Prototype-Selector" class="headerlink" title="原型选择器(Prototype Selector)"></a>原型选择器(Prototype Selector)</h3><p><img src="http://ps1hmyafs.bkt.clouddn.com/image/20190531/Bj1N1dPqytaM.png" alt="mark"></p><p>原型选择器使用<a href="https://lucenenet.apache.org/" target="_blank" rel="noopener">Lucene</a>框架用于检索，并应用其内置算法计算相似度。</p><p><strong>测试阶段</strong></p><p>根据上下文$C$与语料中$C^{‘}$的相似性选出（$C^{‘}$ , $R^{‘}$）</p><p><strong>训练阶段</strong></p><p>由于有真实的response，训练阶段的$C$，$R$对是通过response之间的相似性中选出来的。使用Jaccard相似度，取出范围在0.3-0.7的结果。这样既过滤掉相似度小于0.3的不相关回复，又除去相似度大于0.7的回复，避免直接copy。</p><p>Jaccard相似系数</p><script type="math/tex; mode=display">J(A, B) = \frac{|A \cap B|}{|A \cup B|}</script><p>在这个阶段最终得到多个四元组$(C, R, C^{‘}, R^{‘})$</p><h3 id="上下文感知编辑器（Context-Aware-Neural-Editor）"><a href="#上下文感知编辑器（Context-Aware-Neural-Editor）" class="headerlink" title="上下文感知编辑器（Context-Aware Neural Editor）"></a>上下文感知编辑器（Context-Aware Neural Editor）</h3><h4 id="Edit-Vector-Generation"><a href="#Edit-Vector-Generation" class="headerlink" title="Edit Vector Generation"></a>Edit Vector Generation</h4><p><img src="http://ps1hmyafs.bkt.clouddn.com/image/20190531/i8r6XpJRkoB8.png" alt="mark"></p><p>该部分的主要功能是根据$C$和$C^{‘}$的差异生成编辑向量（edit vector）</p><p>用biGRU对$R^{‘}$进行编码</p><script type="math/tex; mode=display">\overrightarrow{h_j} = f_{GRU}(h_{j-1}, r_{j}^{'}); \overleftarrow{h_j} = f_{GRU}(h_{j+1}, r_{j}^{'})</script><p>用attention机制计算context之间的diff-vector，其中$I=\{w | w \in C \land w \notin C^{‘}\}$表示插入词集合，$I=\{D | w^{‘} \in C^{‘} \land w^{‘} \notin C \}$表示删除词集合，$\oplus$表示concat操作。</p><script type="math/tex; mode=display">diff_{c} = \sum_{w \in I} \beta_w \Psi(w) \oplus \sum_{w^{'} \in D} \gamma_{w^{'}} \Psi(w^{'})</script><script type="math/tex; mode=display">\beta_w = \frac{exp(e_w)}{\sum_{w \in I}exp(e_w)}</script><script type="math/tex; mode=display">e_w = v_{\beta}^{T}tanh(W_{\beta}[\Psi(w)\oplus h_l])</script><script type="math/tex; mode=display">\gamma_{w^{'}} = \frac{exp(e_{w^{'}})}{\sum_{w \in I}exp(e_{w^{'}})}</script><script type="math/tex; mode=display">z = tanh(W \cdot diff_{c} + b)</script><h4 id="Prototype-Editing"><a href="#Prototype-Editing" class="headerlink" title="Prototype Editing"></a>Prototype Editing</h4><p><img src="http://ps1hmyafs.bkt.clouddn.com/image/20190531/TCjGFpwKUP4r.png" alt="mark"></p><p>该部分将编辑向量（edit vector）集成到decoder中进行输出</p><script type="math/tex; mode=display">h_{j}^{\prime}=f_{\mathrm{GRU}}\left(h_{j-1}^{\prime}, r_{j-1} \oplus z_{i}\right)</script><script type="math/tex; mode=display">c_{i}=\sum_{j=1}^{t} \alpha_{i, j} h_{j}</script><script type="math/tex; mode=display">\begin{aligned} \alpha_{i, j} &=\frac{\exp \left(e_{i, j}\right)}{\sum_{k=1}^{t} \exp \left(e_{i, k}\right)} \\ e_{i, j} &=\mathbf{v}^{\top} \tanh \left(\mathbf{W}_{\alpha}\left[h_{j} \oplus h_{i}^{\prime}\right]\right) \end{aligned}</script><script type="math/tex; mode=display">s\left(r_{i}\right)=\operatorname{softmax}\left(\mathbf{W}_{\mathbf{p}}\left[r_{i-1} \oplus h_{i}^{\prime} \oplus c_{i}\right]+\mathbf{b}_{\mathbf{p}}\right)</script><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><script type="math/tex; mode=display">\mathcal{L} = - \sum_{i=1}^N\sum_{j=1}^{l}log p(r_{i,j}|z_i, R_{i}^{'}, r_i, k<j)</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>实验中去掉了长于30个词的context-response对，2000W训练集，1W开发集，1W测试集。</p><p>optimizer: Adam</p><p>batch size: 128</p><p>learning rate: 0.001 </p><p>复杂度连续两个epoch增加就停止训练</p><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>S2SA: Seq2Seq with attention</p><p>S2SA-MMI：bidirectional-MMI</p><p>CVAE:</p><p>Retrieval-default: Luence</p><p>Retrieval-Rerank: dual-LSTM</p><p>Ensemble:  </p><p><img src="http://ps1hmyafs.bkt.clouddn.com/image/20190531/b5a8kWcFkUVR.png" alt="mark"></p><h3 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h3><p><img src="http://ps1hmyafs.bkt.clouddn.com/image/20190531/fApi1vQh1WLv.png" alt="mark"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文针对开放领域回复生成（闲聊）提出了一个先检索<strong>“原型”</strong>再<strong>“编辑”</strong>的范式。以后的工作可能会联合训练<strong>prototype selector</strong>和<strong>neural editor</strong>两个部分。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://ps1hmyafs.bkt.clouddn.com/image/20190531/JBLOFiIj98jN.png&quot; alt=&quot;mark&quot;&gt;&lt;/p&gt;
&lt;p&gt;论文题目：&lt;a href=&quot;https://arxiv.org/abs/1806.0
      
    
    </summary>
    
      <category term="NLP" scheme="http://www.sherlocknlp.com/categories/NLP/"/>
    
      <category term="NLG" scheme="http://www.sherlocknlp.com/categories/NLP/NLG/"/>
    
    
      <category term="dialogue system" scheme="http://www.sherlocknlp.com/tags/dialogue-system/"/>
    
  </entry>
  
  <entry>
    <title>中文NLSQL挑战赛</title>
    <link href="http://www.sherlocknlp.com/%E4%B8%AD%E6%96%87NLSQL%E6%8C%91%E6%88%98%E8%B5%9B/"/>
    <id>http://www.sherlocknlp.com/中文NLSQL挑战赛/</id>
    <published>2019-07-03T11:29:39.881Z</published>
    <updated>2019-07-03T06:34:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://aliyuntianchiresult.cn-hangzhou.oss.aliyun-inc.com/public/files/forum/155954005537218641559540055098.jpeg" alt></p><p>比赛官网：<a href="https://tianchi.aliyun.com/competition/entrance/231716/information" target="_blank" rel="noopener">https://tianchi.aliyun.com/competition/entrance/231716/information</a></p><h2 id="竞赛题目"><a href="#竞赛题目" class="headerlink" title="竞赛题目"></a>竞赛题目</h2><p>首届中文NLSQL挑战赛，使用金融以及通用领域的表格作为数据源，提供在此基础上标注的自然语言与SQL语句的匹配对，目的是设计模型准确将自然语言转换成SQL。</p><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p>本次赛题提供4w条有标签数据作为训练集，1w条无标签数据作为测试集。其中，5千条测试集作为初赛测试集（对选手可见）；另外5千条作为复赛测试集（对选手不可见）。提供的数据集主要由3个文件组成，以训练集为例，包括train.json、train.tables.json及train.db。数据中每一个样本都对应着一个数据表，里面包含该表所有列名，以及相应的数据记录。原则上生成的SQL语句在对应的数据表上是可以执行的，并且都能返回有效的结果。</p><p>train.json</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">     <span class="string">"table_id"</span>: <span class="string">"a1b2c3d4"</span>, <span class="meta"># 相应表格的id</span></span><br><span class="line">     <span class="string">"question"</span>: <span class="string">"世茂茂悦府新盘容积率大于1，请问它的套均面积是多少？"</span>, <span class="meta"># 自然语言问句</span></span><br><span class="line">    <span class="string">"sql"</span>:&#123; <span class="meta"># 真实SQL</span></span><br><span class="line">        <span class="string">"sel"</span>: [<span class="number">7</span>], <span class="meta"># SQL选择的列 </span></span><br><span class="line">        <span class="string">"agg"</span>: [<span class="number">0</span>], <span class="meta"># 选择的列相应的聚合函数, <span class="string">'0'</span>代表无</span></span><br><span class="line">        <span class="string">"cond_conn_op"</span>: <span class="number">0</span>, <span class="meta"># 条件之间的关系</span></span><br><span class="line">        <span class="string">"conds"</span>: [</span><br><span class="line">            [<span class="number">1</span>,<span class="number">2</span>,<span class="string">"世茂茂悦府"</span>], <span class="meta"># 条件列, 条件类型, 条件值，col_1 == <span class="string">"世茂茂悦府"</span></span></span><br><span class="line">            [<span class="number">6</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中，SQL的表达字典说明如下：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">op_sql_dict</span> = &#123;<span class="number">0</span>:<span class="string">"&gt;"</span>, <span class="number">1</span>:<span class="string">"&lt;"</span>, <span class="number">2</span>:<span class="string">"=="</span>, <span class="number">3</span>:<span class="string">"!="</span>&#125;</span><br><span class="line"><span class="attr">agg_sql_dict</span> = &#123;<span class="number">0</span>:<span class="string">""</span>, <span class="number">1</span>:<span class="string">"AVG"</span>, <span class="number">2</span>:<span class="string">"MAX"</span>, <span class="number">3</span>:<span class="string">"MIN"</span>, <span class="number">4</span>:<span class="string">"COUNT"</span>, <span class="number">5</span>:<span class="string">"SUM"</span>&#125;</span><br><span class="line"><span class="attr">conn_sql_dict</span> = &#123;<span class="number">0</span>:<span class="string">""</span>, <span class="number">1</span>:<span class="string">"and"</span>, <span class="number">2</span>:<span class="string">"or"</span>&#125;</span><br></pre></td></tr></table></figure><p>主办方已经将SQL语句做了十分清晰的格式化，比如<code>sel</code>这个字段，其实就是一个多标签分类模型，只不过类别可能会随时变化。<code>agg</code>则跟<code>sel</code>是一一对应的，并且类别是固定的，<code>cond_conn_op</code>则是一个单标签分类问题。</p><p>train.tables.json </p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"id"</span>:<span class="string">"a1b2c3d4"</span>, <span class="meta"># 表格id</span></span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"Table_a1b2c3d4"</span>, <span class="meta"># 表格名称</span></span><br><span class="line">    <span class="string">"title"</span>:<span class="string">"表1：2019年新开工预测 "</span>, <span class="meta"># 表格标题</span></span><br><span class="line">    <span class="string">"header"</span>:[ <span class="meta"># 表格所包含的列名</span></span><br><span class="line">        <span class="string">"300城市土地出让"</span>,</span><br><span class="line">        <span class="string">"规划建筑面积(万㎡)"</span>,</span><br><span class="line">        ……</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"types"</span>:[ <span class="meta"># 表格列所相应的类型</span></span><br><span class="line">        <span class="string">"text"</span>,</span><br><span class="line">        <span class="string">"real"</span>,</span><br><span class="line">        ……</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"rows"</span>:[ <span class="meta"># 表格每一行所存储的值</span></span><br><span class="line">        [<span class="meta"></span></span><br><span class="line"><span class="meta">            <span class="meta-string">"2009年7月-2010年6月"</span>,</span></span><br><span class="line"><span class="meta">            168212.4,</span></span><br><span class="line"><span class="meta">            ……</span></span><br><span class="line"><span class="meta">        </span>]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>tables.db为sqlite格式的数据库形式的表格文件。各个表的表名为tables.json中相应表格的name字段。为避免部分列名中的特殊符号导致无法存入数据库文件，表格中的列名为经过归一化的字段，col_1, col_2, …, col_n。</p><h2 id="baseline"><a href="#baseline" class="headerlink" title="baseline"></a>baseline</h2><h3 id="SQLNet"><a href="#SQLNet" class="headerlink" title="SQLNet"></a>SQLNet</h3><p><img src="http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnXiaAmq9KVAuiaaZLptCv49rQCjezoZa3FkBCb2HwMpuBhmhUaUNe67AnE67hwE18kssWJ7awR7Btg/640?wx_fmt=png" alt></p><p>该模型将生成整个SQL的任务分解为多个子任务，包括select-number，选择哪一列select-column，使用什么聚合函数select-aggregation，有几个条件condition-number，筛选条件针对哪几列condition-column等。</p><p>paper：<a href="https://arxiv.org/pdf/1711.04436.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.04436.pdf</a></p><p>code：<a href="https://github.com/xiaojunxu/SQLNet" target="_blank" rel="noopener">https://github.com/xiaojunxu/SQLNet</a></p><p>该模型使用sketch-based方法，来解决“order-matters”的问题。根据数据集中的数据特征以及编写SQL时的先后顺序，各个子任务之间存在如下图所示的依赖关系。这样的依赖关系可以一定程度上利用已预测好的任务来帮助下游任务更好地预测。</p><p><img src="http://image.sherlocknlp.com/image/20190626/qROywmML3cup.png" alt="mark"></p><h4 id="sequence-to-set"><a href="#sequence-to-set" class="headerlink" title="sequence to set"></a>sequence to set</h4><p>由于出现在WHERE子句中的列名构成了所有列名集合的子集，因此得到where-column的预测概率如下：</p><script type="math/tex; mode=display">P_{\text { wherecol }}(\operatorname{col} | Q)=\sigma\left(u_{c}^{T} E_{c o l}+u_{q}^{T} E_{Q}\right)</script><p>其中$\sigma$是sigmoid激活函数，$E_{c o l}$和$E_{Q}$分别是列名和Query的embedding。</p><h4 id="column-attention"><a href="#column-attention" class="headerlink" title="column attention"></a>column attention</h4><h4 id="OP-slot"><a href="#OP-slot" class="headerlink" title="OP slot"></a>OP slot</h4><h4 id="VALUE-slot"><a href="#VALUE-slot" class="headerlink" title="VALUE slot"></a>VALUE slot</h4><p>感觉<strong>”木桶效应“</strong>的短板就在这里了</p><script type="math/tex; mode=display">{P_{\mathrm{val}}(i | Q, c o l, h)=\operatorname{softmax}(a(h))}</script><script type="math/tex; mode=display">\\ {a(h)_{i}=\left(u^{\mathrm{val}}\right)^{T} \tanh \left(U_{1}^{\mathrm{val}} H_{Q}^{i}+U_{2}^{\mathrm{val}} E_{c o l}+U_{3}^{\mathrm{val}} h\right) \quad \forall i \in\{1, \ldots, L\}}</script><h3 id="SQLova"><a href="#SQLova" class="headerlink" title="SQLova"></a>SQLova</h3><p>SQLova 是韩国 Naver 提出的一种模型，全名为 Search &amp; QLova，是作者们所在部门的名称。此方案是于 SQLNet 的基础上，在模型结构方面做了一些改进而得到的，并没有提出一些创新性的解决方案。</p><p>SQLova 使用了 BERT 来作为模型的输入表达层，代替了词向量。为了让自然语言问句与表结构更好地结合，模型将自然语言问句与列名一并作为输入进行编码。同时，模型在拼接输入时采用了不同的 Segmentation Embedding 来让 BERT 可以区分问题和列名。</p><p><img src="http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnXiaAmq9KVAuiaaZLptCv49rksEzwvBucPfU3DpRGUtZH67MPXibGuguhtnV200Ie6ibrq6iaEe7szOGg/640?wx_fmt=png" alt></p><h3 id="X-SQL"><a href="#X-SQL" class="headerlink" title="X-SQL"></a>X-SQL</h3><p>X-SQL 是微软 Dynamics 365 提出一种方案。这个方案同样继承了解耦任务的思路，将预测 SQL 分解为 6 个子任务。但不同于 SQLova，X-SQL 引入了更多创新性的一些改进，主要包括以下几个方面。 </p><ul><li><p>首先，X-SQL 使用了 MT-DNN 来作为编码层，代替了 SQLova 中使用的 BERT，因为 MT-DNN 在很多其它自然语言处理的下游任务上取得了比 BERT 更好的效果。但因为作者并没有提供对比实验，所以 MT-DNN 可以比 BERT 带来多少的提升就不得而知。 </p></li><li><p>其次，X-SQL 在 SQLova 中原有的 Question Segmentation 和 Column Segmentation 的基础上拓展为 Question、Categorical Column、Numerical Column 以及 Empty Column 这四种 Segmentation Embedding，分别用来作为<strong>自然语言问句</strong>、<strong>文本类型的列</strong>、<strong>数字类型的列</strong>以及<strong>空列</strong>的相应输入。通过引入更多的分类表达，可以让模型得以区分数字与文本类型的列，进而更好地生成 SQL。 </p></li><li><p>最后，在输出层与损失函数部分，Where-Column 的损失函数被定义为了 KL 散度。并且，借助之前提到的引入的特殊列 [EMPTY]，如果模型在预测 Where-Column 时，分数最高的列是 [EMPTY]，那么就无视 Where-Number 所得到的预测结果，判断 SQL 语句为没有条件。 </p></li></ul><p>通过这些输入与结构上的优化，X-SQL 可以在 WikiSQL 的测试集上取得 86.0% 的 Logic Form 准确率和 91.8% 的 Execution 准确率。 </p><h3 id="苏剑林老师基于BERT的baseline"><a href="#苏剑林老师基于BERT的baseline" class="headerlink" title="苏剑林老师基于BERT的baseline"></a>苏剑林老师基于BERT的baseline</h3><p><img src="http://image.sherlocknlp.com/image/20190630/mNz6LiUM0yhC.png" alt="mark"></p><p>代码：<a href="https://github.com/bojone/bert_in_keras/blob/master/nl2sql_baseline.py" target="_blank" rel="noopener">https://github.com/bojone/bert_in_keras/blob/master/nl2sql_baseline.py</a></p><h2 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h2><p><strong>Logic Form Accuracy</strong>: 预测完全正确的SQL语句。其中，列的顺序并不影响准确率的计算。<br><strong>Execution Accuracy</strong>: 预测的SQL的执行结果与真实SQL的执行结果一致。</p><p>计算公式如下：</p><script type="math/tex; mode=display">Score_{l f}=\left\{\begin{array}{ll}{1,} & {S Q L^{\prime}=S Q L} \\ {0,} & {S Q L^{\prime} \neq S Q L}\end{array}\right.</script><script type="math/tex; mode=display">A c c_{l f}=\frac{1}{N} \sum_{n=1}^{N} S c o r e_{l f}^{n}</script><script type="math/tex; mode=display">Score_{e x}=\left\{\begin{array}{ll}{1,} & {Y^{\prime}=Y} \\ {0,} & {Y^{\prime} \neq Y}\end{array}\right.</script><script type="math/tex; mode=display">A c c_{e x}=\frac{1}{N} \sum_{n=1}^{N} S c o r e_{e x}^{n}</script><p>最后线上的评估指标为<strong>（Logic Form Accuracy + Execution Accuracy）/ 2</strong></p><h2 id="其他相似任务"><a href="#其他相似任务" class="headerlink" title="其他相似任务"></a>其他相似任务</h2><p>WikiSQL数据集提供了8w多条有标签数据，足以满足目前的数据驱动型算法对数据量的需求。目前榜单上，效果最好的模型是SQLova和X-SQL，它们在测试集上分别可以达到89.6%和91.8%的执行准确率。</p><p><img src="http://image.sherlocknlp.com/image/20190626/YvKaWleyerov.png" alt="mark"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247497889&amp;idx=1&amp;sn=1abe1d11b147c76f21dc43e3a5cdaf41&amp;chksm=96ea2721a19dae37c3015f9e2ed981f69abf592aa82f03a0f9367ecee4f90f2a5ea707f24cd4&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;key=a9ac41334684e0a1938e8c86e8bfae37fcbb95953f6624213d75bd0eedfbd2685081d8aed77406586738cd1a287540efaa268f65e222d95a1e39ea05781ab3d4465d62c9514bf6b4d9b57ed8c6c42757&amp;ascene=1&amp;uin=MjQzNDM1OTMwMQ%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=cipBWFLJOgqM9Pb08AxN1Z6n0dxWoUUil6zopvkLamsGj2IaFDKkQ%2BW%2B%2FNu5X4HZ" target="_blank" rel="noopener">NL2SQL：弱监督学习与有监督学习完成进阶之路</a></p><p><a href="https://zhuanlan.zhihu.com/p/70408854" target="_blank" rel="noopener">SQLNet——知乎</a></p><p><a href="https://kexue.fm/archives/6771" target="_blank" rel="noopener">基于Bert的NL2SQL模型：一个简明的Baseline</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://aliyuntianchiresult.cn-hangzhou.oss.aliyun-inc.com/public/files/forum/155954005537218641559540055098.jpeg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;比
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>基于 Hexo + Github Pages 搭建个人博客</title>
    <link href="http://www.sherlocknlp.com/%E5%9F%BA%E4%BA%8E%20Hexo%20+%20Github%20Pages%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://www.sherlocknlp.com/基于 Hexo + Github Pages 搭建个人博客/</id>
    <published>2019-06-28T05:57:34.613Z</published>
    <updated>2019-07-03T13:57:31.234Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备知识"><a href="#准备知识" class="headerlink" title="准备知识"></a>准备知识</h2><h3 id="什么是Github-Pages"><a href="#什么是Github-Pages" class="headerlink" title="什么是Github Pages?"></a>什么是Github Pages?</h3><h3 id="什么是Hexo？"><a href="#什么是Hexo？" class="headerlink" title="什么是Hexo？"></a>什么是Hexo？</h3><p><a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo</a>是一个快速、简洁且高效的博客框架。Hexo使用<a href="http://daringfireball.net/projects/markdown/" target="_blank" rel="noopener">Markdown</a>解析文章，在几秒内，即可使用靓丽的主题生成静态网页。</p><h2 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><ul><li>Ubuntu 16.04</li><li><a href="https://git-scm.com/" target="_blank" rel="noopener">git</a></li><li><a href="https://nodejs.org/" target="_blank" rel="noopener">Node.js</a>：建议使用<a href="https://github.com/creationix/nvm" target="_blank" rel="noopener">nvm</a>进行安装</li></ul><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget -qO- https:<span class="regexp">//</span>raw.githubusercontent.com<span class="regexp">/creationix/</span>nvm<span class="regexp">/v0.34.0/i</span>nstall.sh | bash</span><br><span class="line"></span><br><span class="line">nvm install stable</span><br></pre></td></tr></table></figure><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">官方安装文档</a></p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-<span class="keyword">cli</span> -g</span><br></pre></td></tr></table></figure><h3 id="在本地第一次部署"><a href="#在本地第一次部署" class="headerlink" title="在本地第一次部署"></a>在本地第一次部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hexo init blog</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> blog</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo generate</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo server</span></span><br></pre></td></tr></table></figure><p>现在在浏览器中输入localhost:4000</p><ul><li>public: 执行hexo generate命令，输出的静态网页内容目录</li><li>scaffolds: layout模板文件目录</li><li>scripts： 扩展脚本目录，这里可以自定义一些javascript脚本</li><li>source： 文章源码目录，该目录下的markdown和html文件均会被hexo处理</li><li>drafts: 保存草稿文章</li><li>posts:</li><li>_config.yml: 网站的全局配置文件</li><li>package.json: hexo的版本信息</li></ul><h3 id="使用Next主题"><a href="#使用Next主题" class="headerlink" title="使用Next主题"></a>使用Next主题</h3><p>在Hexo中有两份主要的配置文件，其名称都是<code>_config.yml</code>。其中一份位于站点根目录下，主要包含Hexo本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。为了区分，我们将前者称为站点配置文件，后者称为主题配置文件。</p><p>下载<a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">Next</a>主题<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git clone <span class="symbol">https:</span>/<span class="regexp">/github.com/iissnan</span><span class="regexp">/hexo-theme-next themes/next</span></span><br><span class="line"><span class="string">``</span><span class="string">` </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">修改主目录下的_config.yml配置文件</span></span><br></pre></td></tr></table></figure></p><p>theme: next<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在本地测试网站</span><br></pre></td></tr></table></figure></p><p>$ hexo clean<br>$ hexo generate<br>$ hexo server<br>```</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://www.shuang0420.com/2016/05/12/Github-Pages-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">Github Pages+Hexo搭建个人博客</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;准备知识&quot;&gt;&lt;a href=&quot;#准备知识&quot; class=&quot;headerlink&quot; title=&quot;准备知识&quot;&gt;&lt;/a&gt;准备知识&lt;/h2&gt;&lt;h3 id=&quot;什么是Github-Pages&quot;&gt;&lt;a href=&quot;#什么是Github-Pages&quot; class=&quot;header
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《情感对话生成》阅读笔记</title>
    <link href="http://www.sherlocknlp.com/%E3%80%8AEmotional%20Chatting%20Machine%20Emotional%20Conversation%20Generation%20with%20Internal%E3%80%8B/"/>
    <id>http://www.sherlocknlp.com/《Emotional Chatting Machine Emotional Conversation Generation with Internal》/</id>
    <published>2019-06-21T12:30:24.245Z</published>
    <updated>2019-06-21T02:42:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>论文题目：Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory</p><p>论文链接：<a href="https://arxiv.org/abs/1704.01074" target="_blank" rel="noopener">https://arxiv.org/abs/1704.01074</a></p><p>代码链接：<a href="https://github.com/tuxchow/ecm" target="_blank" rel="noopener">https://github.com/tuxchow/ecm</a></p><p>AAAI 2019</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>朱小燕、黄民烈老师团队发布的论文<a href="https://arxiv.org/abs/1704.01074" target="_blank" rel="noopener">「 Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory」</a>首次将情感因素引入了基于深度学习的生成式对话系统，提出了基于记忆网络的情感对话系统 <strong>Emotional Chatting Machine (ECM)</strong> ，在传统的 <strong>Sequence to Sequence</strong> 模型的基础上，ECM 使用了<strong>静态的情感向量嵌入表示</strong>，<strong>动态的情感状态记忆网络</strong>和<strong>情感词外部记忆的机制</strong>，使得 ECM 可以根据用户的输入以及指定情感分类输出相应情感的回复语句。</p><p>代码链接：<a href="https://github.com/tuxchow/ecm" target="_blank" rel="noopener">https://github.com/tuxchow/ecm</a></p><p><img src="https://note.youdao.com/yws/api/personal/file/9920911672A94FE9A3119AFD6253720B?method=download&amp;shareKey=e00d16fb600abf76761f4b92e2c54951" alt="ecm_table1"></p><p>本文用输入作为一种提示和期望的情感的反应，来产生一个回答，采用NLPCC数据集训练一个情感分类器，利用分类器对大规模的对话数据进行自动标注，划分了六类情感，怒、厌恶、快乐、喜欢、悲伤和其他，并用三元数据向量（posts，responses, emotion labels of responses)训练ECM模型。本文设计了一个ECM（Emotional Chatting Machine）框架的，基于记忆网络memory的具有情感生成机制的seq2seq编解码生成模型，模型利用GRU进行编解码。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><img src="https://note.youdao.com/yws/api/personal/file/B280DF312FF744F0A06C31EC5A18947D?method=download&amp;shareKey=12ecdb0a8fa62c1909dfc87c851a6024" alt="model_structure"></p><p>模型的总体框架如上图所示，用户问题输入为“What a lovely day!”，通过 Encoder 将其编码为隐向量表示 h，然后通过注意力机制，结合 decoder 的状态向量 $s$ 在生成不同的词时，对问题的隐向量表示 h 的不同部分的信息选择性的加强，得到向量 $c$。指定情感类别为“Happiness”，经过索引得到情感类别嵌入向量，初始的情感状态记忆向量和相应的情感词表。decoder 接受经过注意力机制的问题向量 $c$，情感类别嵌入向量和初始的情感状态记忆向量作为输入，通过循环神经网络生成下个词的生成概率 o，之后再经过情感词表对情感词和非情感词的加权，得到最终词的生成概率，通过采样即可得到输出“Haha, so happy today!”。</p><h3 id="Encoder-Decoder-Attn框架"><a href="#Encoder-Decoder-Attn框架" class="headerlink" title="Encoder-Decoder-Attn框架"></a>Encoder-Decoder-Attn框架</h3><p><img src="https://note.youdao.com/yws/api/personal/file/F761136507CB4D248E3FFC6CE5F9F4BA?method=download&amp;shareKey=733e71450c39674c12a6ea371be49ac5" alt="attention"></p><p>编码器将输入序列$X=(x_1,x_2,…, x_n)$转化为隐层表示$h=(h_1,h_2,…, h_n)$:</p><script type="math/tex; mode=display">h_t = GRU(h_{t-1}, x_t)</script><p>解码器将上下文向量（context vector）$c_t$和上一个已解码的词向量$e(y_{t-1})$作为输入，使用另一个GRU来更新隐层状态：</p><script type="math/tex; mode=display">s_t = GRU(s_{t-1},[c_t;e(y_{t-1})])</script><p>$[c_t;e(y_{t-1})]$是两个向量的级联，在我们得到隐层向量$s_t$之后，解码器通过计算得到输出概率分布$o_t$生成新的词：</p><script type="math/tex; mode=display">y_t o_t = P(y_t|y_1,y_2,...,y_{t-1},c_t)= softmax(W_os_t)</script><h3 id="Emotional-Category-Embedding"><a href="#Emotional-Category-Embedding" class="headerlink" title="Emotional Category Embedding"></a>Emotional Category Embedding</h3><p>将情感的类别编码成低维度的向量，然后将情感向量$v_e$、向量$e(y_{t-1})$和上下文向量$c_t$合并到一起作为解码器的输入更新解码器的隐层状态$s_t$：</p><script type="math/tex; mode=display">s_{t}=GRU(s_{t-1}, [c_t;e(y_{t-1});v_e])</script><h3 id="Internal-Memory"><a href="#Internal-Memory" class="headerlink" title="Internal Memory"></a>Internal Memory</h3><p>Affect-lm模型论文中提到简单的情感向量嵌入的方式会影响句子语法的准确性。因此，本文设计了一个内部记忆模块在解码时捕捉情感的动态变化。</p><p><img src="https://note.youdao.com/yws/api/personal/file/DA8FDAA8F8B245B1B399018A62045AA9?method=download&amp;shareKey=9a873c20e10345844386f4a69605c2b0" alt="inter_memory"></p><p>本模块受到<a href="https://arxiv.org/abs/1705.04839" target="_blank" rel="noopener">Annotating and modeling empathy in spoken conversations</a>这篇论文启发，具体结构如上图。在每个时间步都会计算Read Gate $g_t^r$和Write Gate $g_t^r$：</p><script type="math/tex; mode=display">g_t^r = sigmoid(W_g^r[e(y_{t-1});s_{t-1};c_t])</script><script type="math/tex; mode=display">g_t^w = sigmoid(W_g^w[e(s_t])</script><p>$g_t^r$和$g_t^w$是用来对内部记忆模块进行读写操作的。这样每一个时间步,情感状态都会被$g_t^w$消除一部分，这与<a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener">Key-value memory networks</a>中<strong>DELETE</strong>操作十分相似。在最后一步时内部的情感信息会被减少至0，表示情感已经被全部表达。</p><script type="math/tex; mode=display">M_{r,t}^I = g_t^r \bigotimes M_{e,t}^I</script><script type="math/tex; mode=display">M_{r,t+1}^I = g_t^w \bigotimes M_{e,t}^I</script><script type="math/tex; mode=display">s_t = GRU(s_{t-1}, [c_t;e(y_{t-1};M_{e,t}^I)])</script><h3 id="External-Memory"><a href="#External-Memory" class="headerlink" title="External Memory"></a>External Memory</h3><p>针对不同的词汇所表达的感情，模型使用External Memory模块来建模显式的情感表达，给情感词汇和普通词汇赋予不同的生成概率。因此，模型可以选择从情感词汇和普通词汇中选择生成一个词。</p><p><img src="https://note.youdao.com/yws/api/personal/file/516AB12A9A254BBAA7D7358FB9974742?method=download&amp;shareKey=e900cb0b52de4cf12c712d6e6309dfe8" alt></p><script type="math/tex; mode=display">\alpha_t = sigmoid(v_u^T s_t)</script><script type="math/tex; mode=display">P_g(y_t=w_g) = softmax(W_g^o s_t)</script><script type="math/tex; mode=display">P_e(y_t=w_e) = softmax(W_e^o s_t)</script><script type="math/tex; mode=display">y_t o_t =P(y_t) = \alpha_t P_e(y_t=w_e) + (1-\alpha_t) P_y(y_t=w_g)</script><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><script type="math/tex; mode=display">L(\theta) = - \sum_{t=1}^{m} p_t log(o_t) - \sum_{t=1}^{m} q_tlog(\alpha_t) + ||M_{e,m}^{I}||</script><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>数据集：<a href="http://coai.cs.tsinghua.edu.cn/hml/challenge2017/" target="_blank" rel="noopener">NLPCC2017 Shared Task 4</a></p><p>情感分类数据集：NLPCC20132 and NLPCC20143（中文数据）<br>The taxonomy comes from <a href="http://tcci.ccf.org.cn/confere-nce/2014/dldoc/evatask1.pdf" target="_blank" rel="noopener">http://tcci.ccf.org.cn/confere-nce/2014/dldoc/evatask1.pdf</a></p><p>大连理工的情感词汇数据集：<a href="http://ir.dlut.edu.cn/news/detail/215" target="_blank" rel="noopener">http://ir.dlut.edu.cn/news/detail/215</a></p><h2 id="主要参考文献"><a href="#主要参考文献" class="headerlink" title="主要参考文献"></a>主要参考文献</h2><ul><li><p>[Ghosh et al. 2017] Ghosh, S.; Chollet, M.; Laksana, E.;<br>Morency, L.; and Scherer, S. 2017. Affect-lm: A neural<br>language model for customizable affective text generation.<br>In ACL, 634–642.</p></li><li><p>[Alam, Danieli, and Riccardi 2017] Alam, F.; Danieli, M.;<br>and Riccardi, G. 2017. Annotating and modeling empathy<br>in spoken conversations. CoRR abs/1705.04839.</p></li><li><p>[Miller et al. 2016] Miller, A. H.; Fisch, A.; Dodge, J.;<br>Karimi, A.; Bordes, A.; and Weston, J. 2016. Keyvalue<br>memory networks for directly reading documents. In<br>EMNLP, 1400–1409.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文题目：Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory&lt;/p&gt;
&lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.o
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>知识驱动的对话生成</title>
    <link href="http://www.sherlocknlp.com/%E7%9F%A5%E8%AF%86%E9%A9%B1%E5%8A%A8%E5%AF%B9%E8%AF%9D/"/>
    <id>http://www.sherlocknlp.com/知识驱动对话/</id>
    <published>2019-05-03T12:57:01.062Z</published>
    <updated>2019-05-26T03:24:14.610Z</updated>
    
    <content type="html"><![CDATA[<p>该任务是基于<a href="http://lic2019.ccf.org.cn/" target="_blank" rel="noopener">2019年语言与智能竞赛</a>，知识驱动对话赛道。当前聊天机器人不够主动，并且回复信息不够丰富。</p><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>给定目标 $g$ 及相关知识信息 $M=f_1, f_2, …, f_n$ ，要求参评的对话系统输出适用于当前新对话序列 $H=u_1, u_2,…,u_{t-1}$ 的机器回复$u_t$使得对话自然流畅、信息丰富而且符合对话目标的规划。在对话过程中，机器处于主动状态，引导用户从一个话题聊到另一个话题。因此，对话系统为机器设定了一个对话目标，$g$为<strong>“START-&gt;TOPIC_A-&gt;TOPIC_B”</strong>，表示从冷启动状态主动聊到话题A，然后进一步聊到话题B。</p><h2 id="比赛数据"><a href="#比赛数据" class="headerlink" title="比赛数据"></a>比赛数据</h2><p>数据中的知识信息来源于电影和娱乐人物领域有价值的信息，如票房、导演、评价等，以三元组SPO的形式组织。对话目标中的话题为电影或娱乐人物实体，数据集中共有3万session，大约12万轮对话，其中10万训练集，1万开发集，1万测试集。</p><p><img src="https://i.loli.net/2019/04/30/5cc7bcb33dbac.jpg" alt="d83d4989ly1g2difnecrtj20sv0gogs7.jpg"></p><h2 id="基线模型"><a href="#基线模型" class="headerlink" title="基线模型"></a>基线模型</h2><p>该模型主要由四部分组成，分别为对话内容编码器、知识编码器、知识管理器和解码器。</p><p>paper:  <a href="https://arxiv.org/abs/1902.04911" target="_blank" rel="noopener">https://arxiv.org/abs/1902.04911</a></p><p>code: <a href="https://github.com/baidu/knowledge-driven-dialogue" target="_blank" rel="noopener">https://github.com/baidu/knowledge-driven-dialogue</a></p><p><img src="https://i.loli.net/2019/04/30/5cc7bd67a99db.jpg" alt="d83d4989ly1g2did035ahj20kv0d6wge.jpg"></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>在这部分，模型使用双向GRU来对<code>对话内容</code>和<code>知识</code>进行编码。我们定义$X$为多轮对话内容，$K$为知识库信息，$Y$为真实的回复。编码器将对话内容 $X$ 和知识信息 $K$ 编码成向量 $x$ 和 $k$，之后送入<strong>Knowledge manager</strong>。编码对话内容和知识信息的两个编码器遵循同样的结构，但不共享参数。</p><script type="math/tex; mode=display">h_t =[\overrightarrow{h}_t ; \overleftarrow{h}_t]=[GRU(x_t, \overrightarrow{h}_{t-1});GRU(x_t, \overleftarrow{h}_{t+1})]</script><h3 id="Knowledge-Manager"><a href="#Knowledge-Manager" class="headerlink" title="Knowledge Manager"></a>Knowledge Manager</h3><p>知识管理模块这部分的作用主要为从外部知识中选出所需知识。在训练过程中，X和Y均被模型作为输入进行训练。这是通过后验信息得到知识的概率分布，计算公式如下：</p><script type="math/tex; mode=display">p(k=k_i |x,y)=\frac{exp(k_i \cdot MLP([x;y]))}{\sum_{j=1}^{N} exp(ki \cdot MLP([x;y]))}</script><p>但在生成预测回复的时候，$Y$对于我们来说是未知的，所以我们只能通过输入$X$来进行知识选择。具体公式如下：</p><script type="math/tex; mode=display">p(k=k_i|x)=\frac{exp(k_i \cdot x)}{\sum_{j=1}^N exp(k_j \cdot x)}</script><p>在训练和预测回复的过程中，选择知识$k$分别是依据后验概率$p(k|x,y)$和$p(k|x)$。因此知识管理模块采用KLDivLoss（KullbackLeibler divergence loss）作为损失函数来衡量先验和后验的相似性。</p><script type="math/tex; mode=display">\mathcal{L}_{KL}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}p(k=k_i|x,y)log\frac{p(k=k_i|x,y)}{p(k=k_i|x)}</script><h3 id="Knowledge-Manager-1"><a href="#Knowledge-Manager-1" class="headerlink" title="Knowledge Manager"></a>Knowledge Manager</h3><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>在上下文内容$c_t$和所选择的$k_i$的条件下，模型的解码器按顺序生成回复。与传统的Seq2Seq解码器不同，该模型将知识融入到回复生成中。因此，模型中介绍两种解码器。</p><p><strong>Standard GRU with Concatenated Inputs</strong></p><p>$s_{t-1}$是GRU的上一步隐层状态，$c_t$是基于attention的上下文向量。</p><script type="math/tex; mode=display">s_t = GRU([y_{t-1};k_i], s_{t-1}, c_t)</script><p><strong>Hierarchical Gated Fusion Unit</strong></p><p>HGFU(Hierarchical Gated Fusion Unit)的中文是分级门控融合单元，其提供了一个相对较“软”的方法将Knowledge合并到回复当中。它主要由3部分组成：utterance GRU、knowledge GRU和fusion unit。</p><p>utterance GRU和knowledge GRU这两部分，均使用标准的GRU结构：</p><script type="math/tex; mode=display">s_t^y = GRU(y_{t-1}, s_{t-1}, c_t)</script><script type="math/tex; mode=display">s_t^k = GRU(k_{i}, s_{t-1}, c_t)</script><p>然后fusion unit将两个隐藏层合并起来生成解码器$t$时刻的隐藏状态。</p><script type="math/tex; mode=display">s_t = r \odot s_t^y + (1-r)\odot s_t^k</script><p>其中，$r$控制着隐层状态$s_t$的分布</p><script type="math/tex; mode=display">r = \sigma(W_z[tanh(W_y s_t^y);tanh(W_k s_t^k)])</script><h3 id="Loss-函数"><a href="#Loss-函数" class="headerlink" title="Loss 函数"></a>Loss 函数</h3><p>除了KLDivLoss以外，模型中的loss函数还包括NLL Loss和BOW Loss。其中，NLL Loss的作用是最小化生成回复与原始回复之间的差异：</p><script type="math/tex; mode=display">\mathcal{L}_{NLL}(\theta)=-\frac{1}{m} \sum_{t=1}^{m} log p_{\theta}(y_t|y<t, x, k_i)</script><p>BOW Loss函数的作用是保证生成的回复和知识之间的相关性</p><script type="math/tex; mode=display">p_{\theta}(y_t|k_i)=\frac{exp(w_{y_t})}{\sum_{v \in V} exp(w_{\theta})}</script><script type="math/tex; mode=display">\mathcal{L}_{BOW}(\theta) = - \frac{1}{m}\sum_{t=1}{m} logp_{\theta}(y_t|k_i)</script><p>因此，该模型的综合损失函数为：</p><script type="math/tex; mode=display">\mathcal{L}(\theta) = \mathcal{L}_{KL}(\theta)+\mathcal{L}_{NLL}(\theta)+\mathcal{L}_{BOW}(\theta)</script><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><h3 id="自动评价指标"><a href="#自动评价指标" class="headerlink" title="自动评价指标"></a>自动评价指标</h3><ul><li>F1：评估输出回复相对于标准回复在自己别上的准确召回性能，是评估模型性能的主要指标；</li><li>BLEU：评估输出回复相对于标准回复在词级别上的性能，是评估模型性能的辅助指标；</li><li>DISTINCT：评估输出回复的多样性，是评估模型性能的辅助指标；</li></ul><h3 id="人工评价"><a href="#人工评价" class="headerlink" title="人工评价"></a>人工评价</h3><p>人工评估从流畅性、一致性和主动性等几个维度进行评估。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;该任务是基于&lt;a href=&quot;http://lic2019.ccf.org.cn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2019年语言与智能竞赛&lt;/a&gt;，知识驱动对话赛道。当前聊天机器人不够主动，并且回复信息不够丰富。&lt;/p&gt;
&lt;h2 id=&quot;任
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://www.sherlocknlp.com/hello-world/"/>
    <id>http://www.sherlocknlp.com/hello-world/</id>
    <published>2019-04-08T03:06:35.834Z</published>
    <updated>2019-04-08T03:05:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
