<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《情感对话生成》阅读笔记]]></title>
    <url>%2F%E3%80%8AEmotional%20Chatting%20Machine%20Emotional%20Conversation%20Generation%20with%20Internal%E3%80%8B%2F</url>
    <content type="text"><![CDATA[论文题目：Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory 论文链接：https://arxiv.org/abs/1704.01074 代码链接：https://github.com/tuxchow/ecm AAAI 2019 Introduction朱小燕、黄民烈老师团队发布的论文「 Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory」首次将情感因素引入了基于深度学习的生成式对话系统，提出了基于记忆网络的情感对话系统 Emotional Chatting Machine (ECM) ，在传统的 Sequence to Sequence 模型的基础上，ECM 使用了静态的情感向量嵌入表示，动态的情感状态记忆网络和情感词外部记忆的机制，使得 ECM 可以根据用户的输入以及指定情感分类输出相应情感的回复语句。 代码链接：https://github.com/tuxchow/ecm 本文用输入作为一种提示和期望的情感的反应，来产生一个回答，采用NLPCC数据集训练一个情感分类器，利用分类器对大规模的对话数据进行自动标注，划分了六类情感，怒、厌恶、快乐、喜欢、悲伤和其他，并用三元数据向量（posts，responses, emotion labels of responses)训练ECM模型。本文设计了一个ECM（Emotional Chatting Machine）框架的，基于记忆网络memory的具有情感生成机制的seq2seq编解码生成模型，模型利用GRU进行编解码。 Model 模型的总体框架如上图所示，用户问题输入为“What a lovely day!”，通过 Encoder 将其编码为隐向量表示 h，然后通过注意力机制，结合 decoder 的状态向量 $s$ 在生成不同的词时，对问题的隐向量表示 h 的不同部分的信息选择性的加强，得到向量 $c$。指定情感类别为“Happiness”，经过索引得到情感类别嵌入向量，初始的情感状态记忆向量和相应的情感词表。decoder 接受经过注意力机制的问题向量 $c$，情感类别嵌入向量和初始的情感状态记忆向量作为输入，通过循环神经网络生成下个词的生成概率 o，之后再经过情感词表对情感词和非情感词的加权，得到最终词的生成概率，通过采样即可得到输出“Haha, so happy today!”。 Encoder-Decoder-Attn框架 编码器将输入序列$X=(x_1,x_2,…, x_n)$转化为隐层表示$h=(h_1,h_2,…, h_n)$: h_t = GRU(h_{t-1}, x_t)解码器将上下文向量（context vector）$c_t$和上一个已解码的词向量$e(y_{t-1})$作为输入，使用另一个GRU来更新隐层状态： s_t = GRU(s_{t-1},[c_t;e(y_{t-1})])$[c_t;e(y_{t-1})]$是两个向量的级联，在我们得到隐层向量$s_t$之后，解码器通过计算得到输出概率分布$o_t$生成新的词： y_t o_t = P(y_t|y_1,y_2,...,y_{t-1},c_t) = softmax(W_os_t)Emotional Category Embedding将情感的类别编码成低维度的向量，然后将情感向量$v_e$、向量$e(y_{t-1})$和上下文向量$c_t$合并到一起作为解码器的输入更新解码器的隐层状态$s_t$： s_{t}=GRU(s_{t-1}, [c_t;e(y_{t-1});v_e])Internal MemoryAffect-lm模型论文中提到简单的情感向量嵌入的方式会影响句子语法的准确性。因此，本文设计了一个内部记忆模块在解码时捕捉情感的动态变化。 本模块受到Annotating and modeling empathy in spoken conversations这篇论文启发，具体结构如上图。在每个时间步都会计算Read Gate $g_t^r$和Write Gate $g_t^r$： g_t^r = sigmoid(W_g^r[e(y_{t-1});s_{t-1};c_t]) g_t^w = sigmoid(W_g^w[e(s_t])$g_t^r$和$g_t^w$是用来对内部记忆模块进行读写操作的。这样每一个时间步,情感状态都会被$g_t^w$消除一部分，这与Key-value memory networks中DELETE操作十分相似。在最后一步时内部的情感信息会被减少至0，表示情感已经被全部表达。 M_{r,t}^I = g_t^r \bigotimes M_{e,t}^I M_{r,t+1}^I = g_t^w \bigotimes M_{e,t}^I s_t = GRU(s_{t-1}, [c_t;e(y_{t-1};M_{e,t}^I)])External Memory针对不同的词汇所表达的感情，模型使用External Memory模块来建模显式的情感表达，给情感词汇和普通词汇赋予不同的生成概率。因此，模型可以选择从情感词汇和普通词汇中选择生成一个词。 \alpha_t = sigmoid(v_u^T s_t) P_g(y_t=w_g) = softmax(W_g^o s_t) P_e(y_t=w_e) = softmax(W_e^o s_t) y_t o_t =P(y_t) = \alpha_t P_e(y_t=w_e) + (1-\alpha_t) P_y(y_t=w_g)损失函数 L(\theta) = - \sum_{t=1}^{m} p_t log(o_t) - \sum_{t=1}^{m} q_tlog(\alpha_t) + ||M_{e,m}^{I}||数据集数据集：NLPCC2017 Shared Task 4 情感分类数据集：NLPCC20132 and NLPCC20143（中文数据）The taxonomy comes from http://tcci.ccf.org.cn/confere-nce/2014/dldoc/evatask1.pdf 大连理工的情感词汇数据集：http://ir.dlut.edu.cn/news/detail/215 主要参考文献 [Ghosh et al. 2017] Ghosh, S.; Chollet, M.; Laksana, E.;Morency, L.; and Scherer, S. 2017. Affect-lm: A neurallanguage model for customizable affective text generation.In ACL, 634–642. [Alam, Danieli, and Riccardi 2017] Alam, F.; Danieli, M.;and Riccardi, G. 2017. Annotating and modeling empathyin spoken conversations. CoRR abs/1705.04839. [Miller et al. 2016] Miller, A. H.; Fisch, A.; Dodge, J.;Karimi, A.; Bordes, A.; and Weston, J. 2016. Keyvaluememory networks for directly reading documents. InEMNLP, 1400–1409.]]></content>
  </entry>
  <entry>
    <title><![CDATA[《通过对响应原型编辑的回复生成》阅读笔记]]></title>
    <url>%2F%E3%80%8A%E9%80%9A%E8%BF%87%E5%AF%B9%E5%93%8D%E5%BA%94%E5%8E%9F%E5%9E%8B%E7%BC%96%E8%BE%91%E7%9A%84%E5%9B%9E%E5%A4%8D%E7%94%9F%E6%88%90%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文题目：Response Generation by Context-aware Prototype Editing 代码链接：https://github.com/MarkWuNLP/ResponseEdit AAAI 2019 北航 本文的主要目的是为了解决开放域对话生成中，生成句子较短且无意义的问题。模型首先从预定义的索引中检索到响应，然后根据问题原型和当前问题的差异编辑响应原型。主要思想是基于由于响应原型有很好的语法和信息，对响应原型进行轻微地调整就能得到较好的回复这一假设，来进行相关实验和论证。这是一种“检索+生成”的方法，既有检索式聊天机器人回复流畅和信息量大的优势，还有生成式聊天机器人的灵活性和相关性。 模型该模型由原型选择器$\mathcal{S}$和上下文感知编辑器$\mathcal{E}$组成。给定一个新的会话，我们首先使用选择器S来检索语料$\mathcal{D} = {(C_i, R_i)}_{i=1}^{N}$中的$(C_{i}, R_{i}) \in \mathcal{D}$。然后编辑器$\mathcal{E}$计算出$z_i = f(C_i, C)$来编码$C_i$和$C$之间的差异信息。最后，通过概率$p(R|z_i, R_i)$生成回复。 原型选择器(Prototype Selector) 原型选择器使用Lucene框架用于检索，并应用其内置算法计算相似度。 测试阶段 根据上下文$C$与语料中$C^{‘}$的相似性选出（$C^{‘}$ , $R^{‘}$） 训练阶段 由于有真实的response，训练阶段的$C$，$R$对是通过response之间的相似性中选出来的。使用Jaccard相似度，取出范围在0.3-0.7的结果。这样既过滤掉相似度小于0.3的不相关回复，又除去相似度大于0.7的回复，避免直接copy。 Jaccard相似系数 J(A, B) = \frac{|A \cap B|}{|A \cup B|}在这个阶段最终得到多个四元组$(C, R, C^{‘}, R^{‘})$ 上下文感知编辑器（Context-Aware Neural Editor）Edit Vector Generation 该部分的主要功能是根据$C$和$C^{‘}$的差异生成编辑向量（edit vector） 用biGRU对$R^{‘}$进行编码 \overrightarrow{h_j} = f_{GRU}(h_{j-1}, r_{j}^{'}); \overleftarrow{h_j} = f_{GRU}(h_{j+1}, r_{j}^{'})用attention机制计算context之间的diff-vector，其中$I=\{w | w \in C \land w \notin C^{‘}\}$表示插入词集合，$I=\{D | w^{‘} \in C^{‘} \land w^{‘} \notin C \}$表示删除词集合，$\oplus$表示concat操作。 diff_{c} = \sum_{w \in I} \beta_w \Psi(w) \oplus \sum_{w^{'} \in D} \gamma_{w^{'}} \Psi(w^{'}) \beta_w = \frac{exp(e_w)}{\sum_{w \in I}exp(e_w)} e_w = v_{\beta}^{T}tanh(W_{\beta}[\Psi(w)\oplus h_l]) \gamma_{w^{'}} = \frac{exp(e_{w^{'}})}{\sum_{w \in I}exp(e_{w^{'}})} z = tanh(W \cdot diff_{c} + b)Prototype Editing 该部分将编辑向量（edit vector）集成到decoder中进行输出 h_{j}^{\prime}=f_{\mathrm{GRU}}\left(h_{j-1}^{\prime}, r_{j-1} \oplus z_{i}\right) c_{i}=\sum_{j=1}^{t} \alpha_{i, j} h_{j} \begin{aligned} \alpha_{i, j} &=\frac{\exp \left(e_{i, j}\right)}{\sum_{k=1}^{t} \exp \left(e_{i, k}\right)} \\ e_{i, j} &=\mathbf{v}^{\top} \tanh \left(\mathbf{W}_{\alpha}\left[h_{j} \oplus h_{i}^{\prime}\right]\right) \end{aligned} s\left(r_{i}\right)=\operatorname{softmax}\left(\mathbf{W}_{\mathbf{p}}\left[r_{i-1} \oplus h_{i}^{\prime} \oplus c_{i}\right]+\mathbf{b}_{\mathbf{p}}\right)目标函数 \mathcal{L} = - \sum_{i=1}^N\sum_{j=1}^{l}log p(r_{i,j}|z_i, R_{i}^{'}, r_i, k]]></content>
  </entry>
  <entry>
    <title><![CDATA[知识驱动的对话生成]]></title>
    <url>%2F%E7%9F%A5%E8%AF%86%E9%A9%B1%E5%8A%A8%E5%AF%B9%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[该任务是基于2019年语言与智能竞赛，知识驱动对话赛道。当前聊天机器人不够主动，并且回复信息不够丰富。 任务给定目标 $g$ 及相关知识信息 $M=f_1, f_2, …, f_n$ ，要求参评的对话系统输出适用于当前新对话序列 $H=u_1, u_2,…,u_{t-1}$ 的机器回复$u_t$使得对话自然流畅、信息丰富而且符合对话目标的规划。在对话过程中，机器处于主动状态，引导用户从一个话题聊到另一个话题。因此，对话系统为机器设定了一个对话目标，$g$为“START-&gt;TOPIC_A-&gt;TOPIC_B”，表示从冷启动状态主动聊到话题A，然后进一步聊到话题B。 比赛数据数据中的知识信息来源于电影和娱乐人物领域有价值的信息，如票房、导演、评价等，以三元组SPO的形式组织。对话目标中的话题为电影或娱乐人物实体，数据集中共有3万session，大约12万轮对话，其中10万训练集，1万开发集，1万测试集。 基线模型该模型主要由四部分组成，分别为对话内容编码器、知识编码器、知识管理器和解码器。 paper: https://arxiv.org/abs/1902.04911 code: https://github.com/baidu/knowledge-driven-dialogue Encoder在这部分，模型使用双向GRU来对对话内容和知识进行编码。我们定义$X$为多轮对话内容，$K$为知识库信息，$Y$为真实的回复。编码器将对话内容 $X$ 和知识信息 $K$ 编码成向量 $x$ 和 $k$，之后送入Knowledge manager。编码对话内容和知识信息的两个编码器遵循同样的结构，但不共享参数。 h_t =[\overrightarrow{h}_t ; \overleftarrow{h}_t]=[GRU(x_t, \overrightarrow{h}_{t-1});GRU(x_t, \overleftarrow{h}_{t+1})]Knowledge Manager知识管理模块这部分的作用主要为从外部知识中选出所需知识。在训练过程中，X和Y均被模型作为输入进行训练。这是通过后验信息得到知识的概率分布，计算公式如下： p(k=k_i |x,y)=\frac{exp(k_i \cdot MLP([x;y]))}{\sum_{j=1}^{N} exp(ki \cdot MLP([x;y]))}但在生成预测回复的时候，$Y$对于我们来说是未知的，所以我们只能通过输入$X$来进行知识选择。具体公式如下： p(k=k_i|x)=\frac{exp(k_i \cdot x)}{\sum_{j=1}^N exp(k_j \cdot x)}在训练和预测回复的过程中，选择知识$k$分别是依据后验概率$p(k|x,y)$和$p(k|x)$。因此知识管理模块采用KLDivLoss（KullbackLeibler divergence loss）作为损失函数来衡量先验和后验的相似性。 \mathcal{L}_{KL}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}p(k=k_i|x,y)log\frac{p(k=k_i|x,y)}{p(k=k_i|x)}Knowledge ManagerDecoder在上下文内容$c_t$和所选择的$k_i$的条件下，模型的解码器按顺序生成回复。与传统的Seq2Seq解码器不同，该模型将知识融入到回复生成中。因此，模型中介绍两种解码器。 Standard GRU with Concatenated Inputs $s_{t-1}$是GRU的上一步隐层状态，$c_t$是基于attention的上下文向量。 s_t = GRU([y_{t-1};k_i], s_{t-1}, c_t)Hierarchical Gated Fusion Unit HGFU(Hierarchical Gated Fusion Unit)的中文是分级门控融合单元，其提供了一个相对较“软”的方法将Knowledge合并到回复当中。它主要由3部分组成：utterance GRU、knowledge GRU和fusion unit。 utterance GRU和knowledge GRU这两部分，均使用标准的GRU结构： s_t^y = GRU(y_{t-1}, s_{t-1}, c_t) s_t^k = GRU(k_{i}, s_{t-1}, c_t)然后fusion unit将两个隐藏层合并起来生成解码器$t$时刻的隐藏状态。 s_t = r \odot s_t^y + (1-r)\odot s_t^k其中，$r$控制着隐层状态$s_t$的分布 r = \sigma(W_z[tanh(W_y s_t^y);tanh(W_k s_t^k)])Loss 函数除了KLDivLoss以外，模型中的loss函数还包括NLL Loss和BOW Loss。其中，NLL Loss的作用是最小化生成回复与原始回复之间的差异： \mathcal{L}_{NLL}(\theta)=-\frac{1}{m} \sum_{t=1}^{m} log p_{\theta}(y_t|y]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
