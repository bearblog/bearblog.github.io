<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[NLP文本表示预训练技术]]></title>
    <url>%2FNLP%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[NLP里程碑 时间 事件 2001 Neural Language Model 2008 Multi-task learning 2013 Word Embedding / NN for NLP 2014 Sequence to Sequence 2015 Attention 2016 Memory Network 2018 Pretrained Language Model 通常来说，NLP 中监督任务的基本套路都可以用三个积木来进行归纳： 文本数据搜集和预处理 将文本进行编码和表征 设计模型解决具体任务 文本表示如何通过量化的方式来表示一个单词是NLP领域非常核心的问题。 文本表示的方法 基于one-hot、tfidf、textrank等的bag-of-words; 主题模型：LSA、pLSA、LDA; 基于词向量的静态表示：word2vec、fastText、glove; 基于词向量的动态表示：elmo、gpt、bert 语言表征学习 nlp在深度学习的基本单元是向量$\{x_1, x_2, …, x_n\}$，然后通过变换、整合得到新的向量h——表征（Representation），再基于向量h得到对输出的判断y。 分布式语义假设（Distributional Hypothesis） One shall know a word by the company it keeps. 语言模型语言模型是对一段文本的概率进行估计即针对文本$X$，计算$P(X)$的概率，语言模型分为统计语言模型和神经网络语言模型。 统计语言模型/n-gram模型语言模型的本质是对一段文本(词序列)进行预测概率的分布，即如果文本用 $X$ 来表示，那么语言模型就是要求 P(X_i)的大小。通俗的来讲，最后得到的概率越大说明越合理，概率越小越不合理（不是人话）。按照大数定律中频率对于概率无限逼近的思想，通过计算这个文本在所有人类历史上产生过的所有文本集合中的频率 $P(X_i)$ ，公式如下： P(X_i)=\frac{c(X_i)}{\sum_{i=0}^{\infty} c(X_i)}该公式的问题是全人类所有历史语料的这种统计显然无法实现。因此我们将文本拆成词，通过词之间的概率关系，求得整个文本的概率大小。假定句子长度为T，词用x表示，即： P(X) = P(x_0, x_1,...,x_T)=P(x_0)*P(x_1|x_0)*P(x_2|x_0,x_1)...P(x_T|x_0,x_1,...,x_{T-1}) 但是上式仍旧复杂，我们一般会引入马尔科夫假设：假定一个句子中词只与它前面的 n 个词相关。特别地，当n=1时句子的计算概率如下： P(X) = P(x_0, x_1,...,x_T)=P(x_0)P(x_1|x_0)P(x_2|x_1)...P(x_T|x_{T-1})=P(x_0)\prod_{i=0}^{T-1}P(x_{i+1}|x_{i})什么是马尔科夫假设？ 马尔科夫假设是指，每个词出现的概率只跟它前面的少数几个词有关。比如，二阶马尔科夫假设只考虑前面两个词，响应的语言模型是三元模型（3-gram）。引入了马尔科夫假设的语言模型，也可以叫做马克科夫模型。 然而，基于统计的语言模型存在着很多问题。 神经网络语言模型（NNLM）2003年Bengio在他的经典论文 A Neural Probabilistic Language Model 中，首次将深度学习的思想融入语言模型中，并发现将训练得到的NNLM（Neural Net Language Model, 神经网络语言模型）的第一层参数当作词的分布式表征（每个词表示为稠密的实数向量）时，能很好地获取词语之间的相似度。NNLM模型的目标是构建语言模型，而词向量只是一个副产物，其目标函数如下： {L=\frac{1}{T} \sum_{t} \log P\left(w_{t} | w_{t-1}, \ldots, w_{t-n+1} ; \theta\right)+R(\theta)} \\ {P\left(w_{t} | w_{t-1}, \ldots, w_{t-n+1} ; \theta\right)=\frac{e^{y_{i t}}}{\sum_{i} e^{y_{i}}}} NNLM的主要贡献是将模型的第一次层特征映射矩阵当作词的分布式表示，从而可以将一个词表示为一个向量形式。这直接启发了word2vec的工作。 NNLM的问题： NNLM虽然将N-Gram的阶数n提高到了5，相比原来的统计语言模型进步很大，但这个级别的长程依赖关系仍不够。而且NNLM只对词的左侧文本进行建模，所以得到的词向量并不是语境的充分表征。还有一个比较严重的问题就是NNLM的训练太慢。 自然语言预训练技术Word2vec2013年，Tomas Mikolov连放几篇划时代的论文，其中最为重要的是两篇：Efficient Estimation of Word Representations in Vector Space（一文首次提出CBOW和Skip-gram模型）和Distributed Representations of Words and Phrases and their Compositionality(文中介绍了几种优化训练方法，包括Hierarchical Softmax)。鼎鼎有名的word2vec只是一个工具，背后的模型是CBOW和Skip-gram，并且使用了Hierarchical Softmax或Negative Sampling这些训练的优化方法。 $Word2Vec$简单讲其实就是通过学习文本，来用词向量的方式表征词的语义信息，即通过Embedding把原先词所在空间映射到一个新的空间中去，使得语义上相似的单词在该空间内距离相近。 CBOWCBOW的全称是Continuous Bag-of-words，也就是连续的词袋模型，其目标函数如下： 首先，CBOW没有隐藏层，本质上只有两层结构。从这可以看出，CBOW有三个特点： 第一，取消了NNLM中的隐藏层，直接将输入层和输出层相连； 第二，在求语境context向量时，语境内的词序已经丢失； 第三，因为最终的目标函数仍然是语言模型的目标函数，所以需要顺序遍历预料中的每一个词。 需要注意的是这里每个词对应到两个词向量，其中$e(w_t)$是词的输入向量，而$e^{‘}(w_t)$则是词的输出向量。 Skip-gram word2vec工具的使用源码地址：https://code.google.com/archive/p/word2vec/ 下载源码后运行make进行编译： 训练命令： 1./word2vec -train cutted_text.txt -output vectors.bin -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1 聚类： 1./word2vec -train cutted_text.txt -output classes.txt -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -classes 500 按类别排序： 1sort classes.txt -k 2 -n &gt; classes.sorted.txt GloveGlove是一种用于获取单词向量表征的无监督学习算法。 目前将预训练语言表征应用于下游任务存在两种策略：基于特征的策略和微调的策略（fine-tuning）。基于特征的策略（如ELMo）使用将预训练的表征作为额外特征。而微调策略（GPT），通过简单地微调预训练参数在下游任务中进行训练。 CoVe ELMo2018年的早些时候吧，AllenNLP的Matthew E. Peters 等人在论文Deep contextualized word representations中首次提出ELMo，它的全称是Embeddings from Language Models。对于一个单词，与word2vec或GLoVe等传统词嵌入不同，ELMo会动态地学出上下文中的词向量。 目标函数： $\sum_{k=1}^{N}\left(\log p\left(t_{k} | t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \vec{\Theta}_{L S T M}, \Theta_{s}\right)\right.\left.+\log p\left(t_{k} | t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right)$ ELMo通过将隐藏状态和初始嵌入以加权求平均，得到语境化词嵌入。 \text { ELMo }_{k}^{\text {task }}=E\left(R_{k} ; \Theta^{\text {task }}\right)=\gamma^{\text {task }} \sum_{j=0}^{L} s_{j}^{\text {task }} \mathbf{h}_{k, j}^{L N}GPT https://transformer.huggingface.co/ BERT BERT是一种基于transformer架构的预训练语言表示方法，这意味着我们在大型文本语料库（如维基百科）上训练通用的“语言理解”模型，然后将该模型用于我们关心的下游NLP任务（如问答）。 BERT优于以前的方法，因为它是第一个用于预训练NLP的无监督，深度双向系统。 BERT联合遮蔽语言模型（masked language model, MLM）和下一句预测（next sentence prediction）两个任务联合预训练文本的表征。 任务1：Masked LM为了训练深度双向表征，BERT模型采取了一个直接的方法，随机遮蔽输入token的某些部分，然后预测被遮住的token。该步骤在文献中通常被称为Cloze任务（Taylor, 1953）。在这种情况下，对应遮蔽的token的最终隐藏向量会输入到softmax函数中，并如标准的LM那样预测所有词汇的概率。在实验中，随机遮住每个序列15%的token。与去噪自编码器（Vincent et al., 2008）相反，我们仅预测遮蔽单词而非重建整个输入。 任务2：下一句预测很多重要的下游任务（如问答QA和自然语言推断NLI）基于对两个文本句子之间关系的理解。 BERT最主要的几个特征： 利用了“真”双向的Transformer; 为了利用双向信息，改进了普通语言模型为完形填空式的Mask-LM； 利用Next Sentence Prediction任务学习句子级别的信息； 进一步完善和扩展了GPT中设计的通用框架；使得BERT能够支持包括：句子对分类任务、单句子分类任务、阅读理解任务和序列标注任务。 如何使用BERT使用BERT有两个阶段：预训练（Pre-training）和微调（Fine-tuning）。 预训练费用相当昂贵（4到16个云TPU为4天）。大多数NLP研究人员不需要从头开始训练他们自己的模型。 微调取决于下游的具体任务，不同的下游任务意味着不同的网络扩展结构。总的来说，对BERT的微调是一个轻量级任务，微调主要调整的是扩展网络而不是BERT本身。换句话说，我们完全可以固定住BERT的参数，把BERT输出的向量编码当做一个特征信息，用于各种下游任务。 https://jalammar.github.io/illustrated-bert/ 将学习转移到下游任务 使用Pytorch版本BERT的使用方式 1）First prepare a tokenized input with BertTokenizer 1234567891011121314151617181920212223import torchfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM # 加载词典 pre-trained model tokenizer (vocabulary)tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Tokenized inputtext = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"tokenized_text = tokenizer.tokenize(text) # Mask a token that we will try to predict back with `BertForMaskedLM`masked_index = 8tokenized_text[masked_index] = '[MASK]'assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]'] # 将 token 转为 vocabulary 索引indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)# 定义句子 A、B 索引segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] # 将 inputs 转为 PyTorch tensorstokens_tensor = torch.tensor([indexed_tokens])segments_tensors = torch.tensor([segments_ids]) 2）use BertModel to get hidden states 1234567891011121314# 加载模型 pre-trained model (weights)model = BertModel.from_pretrained('bert-base-uncased')model.eval() # GPU &amp; put everything on cudatokens_tensor = tokens_tensor.to('cuda')segments_tensors = segments_tensors.to('cuda')model.to('cuda') # 得到每一层的 hidden states with torch.no_grad(): encoded_layers, _ = model(tokens_tensor, segments_tensors)# 模型 bert-base-uncased 有12层，所以 hidden states 也有12层assert len(encoded_layers) == 12 两行代码玩转Google句向量和词向量 XLNet参考干货|全面理解N-Gram语言模型 深度长文：NLP的巨人肩膀（上） NLP 的巨人肩膀（下）：从 CoVe 到 BERT 从Word Embedding到Bert模型——自然语言处理预训练技术发展史 ELMo解读（论文 + PyTorch源码） 图解2018年领先的两大NLP模型：BERT和ELMo 自然语言处理中的语言模型预训练方法 NLP中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert BERT——CSDN 解密BERT Pytorch版本的BERT使用学习笔记 两行代码代码玩转Google BERT局向量词向量 Bert时代的创新：Bert在NLP各领域的应用进展 BERT时代与后时代的NLP（一） BERT时代与后时代的NLP（二） 拆解XLNet模型设计，回顾语言表征学习的思想演进 Dissecting BERT Part 1: The Encoder]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天池——中文NLSQL挑战赛]]></title>
    <url>%2F%E4%B8%AD%E6%96%87NLSQL%E6%8C%91%E6%88%98%E8%B5%9B%2F</url>
    <content type="text"><![CDATA[比赛官网：https://tianchi.aliyun.com/competition/entrance/231716/information 竞赛题目首届中文NLSQL挑战赛，使用金融以及通用领域的表格作为数据源，提供在此基础上标注的自然语言与SQL语句的匹配对，目的是设计模型准确将自然语言描述转换成对应的SQL查询语句（即Natural Language to SQL/Text to SQL）。目前该技术面临以下挑战：泛化性（跨领域）、复杂性（SQL语法）、正确性（问题和表格的对齐关系）。 数据样例本次赛题提供4w条有标签数据作为训练集，1w条无标签数据作为测试集。其中，5千条测试集作为初赛测试集（对选手可见）；另外5千条作为复赛测试集（对选手不可见）。提供的数据集主要由3个文件组成，以训练集为例，包括train.json、train.tables.json及train.db。数据中每一个样本都对应着一个数据表，里面包含该表所有列名，以及相应的数据记录。原则上生成的SQL语句在对应的数据表上是可以执行的，并且都能返回有效的结果。 train.json 12345678910111213&#123; "table_id": "a1b2c3d4", # 相应表格的id "question": "世茂茂悦府新盘容积率大于1，请问它的套均面积是多少？", # 自然语言问句 "sql":&#123; # 真实SQL "sel": [7], # SQL选择的列 "agg": [0], # 选择的列相应的聚合函数, '0'代表无 "cond_conn_op": 0, # 条件之间的关系 "conds": [ [1,2,"世茂茂悦府"], # 条件列, 条件类型, 条件值，col_1 == "世茂茂悦府" [6,0,1] ] &#125;&#125; 其中，SQL的表达字典说明如下： 123op_sql_dict = &#123;0:"&gt;", 1:"&lt;", 2:"==", 3:"!="&#125;agg_sql_dict = &#123;0:"", 1:"AVG", 2:"MAX", 3:"MIN", 4:"COUNT", 5:"SUM"&#125;conn_sql_dict = &#123;0:"", 1:"and", 2:"or"&#125; 主办方已经将SQL语句做了十分清晰的格式化，比如sel这个字段，其实就是一个多标签分类模型，只不过类别可能会随时变化。agg则跟sel是一一对应的，并且类别是固定的，cond_conn_op则是一个单标签分类问题。 train.tables.json 12345678910111213141516171819202122&#123; "id":"a1b2c3d4", # 表格id "name":"Table_a1b2c3d4", # 表格名称 "title":"表1：2019年新开工预测 ", # 表格标题 "header":[ # 表格所包含的列名 "300城市土地出让", "规划建筑面积(万㎡)", …… ], "types":[ # 表格列所相应的类型 "text", "real", …… ], "rows":[ # 表格每一行所存储的值 [ "2009年7月-2010年6月", 168212.4, …… ] ]&#125; tables.db为sqlite格式的数据库形式的表格文件。各个表的表名为tables.json中相应表格的name字段。为避免部分列名中的特殊符号导致无法存入数据库文件，表格中的列名为经过归一化的字段，col_1, col_2, …, col_n。 BaselineSQLNet 该模型将生成整个SQL的任务分解为多个子任务，包括select-number，选择哪一列select-column，使用什么聚合函数select-aggregation，有几个条件condition-number，筛选条件针对哪几列condition-column等。 paper：https://arxiv.org/pdf/1711.04436.pdf code：https://github.com/xiaojunxu/SQLNet 该模型使用sketch-based方法，来解决“order-matters”的问题。根据数据集中的数据特征以及编写SQL时的先后顺序，各个子任务之间存在如下图所示的依赖关系。这样的依赖关系可以一定程度上利用已预测好的任务来帮助下游任务更好地预测。 预处理：schema linking sequence to set由于出现在WHERE子句中的列名构成了所有列名集合的子集，因此得到where-column的预测概率如下： P_{\text { wherecol }}(\operatorname{col} | Q)=\sigma\left(u_{c}^{T} E_{c o l}+u_{q}^{T} E_{Q}\right)其中$\sigma$是sigmoid激活函数，$E_{c o l}$和$E_{Q}$分别是列名和Query的embedding，它们分别是由两个不共享权重的BiLSTM的最后一个隐层状态表示的。 column attention v_{i}=\left(E_{c o l}\right)^{T} W H_{Q}^{i} w=\operatorname{softmax}(v) E_{Q | c o l}=H_{Q} w P_{\text {wherecol}}(\operatorname{col} | Q)=\sigma\left(\left(u_{a}^{\text {col}}\right)^{T} \tanh \left(U_{c}^{\operatorname{col}} E_{c o l}+U_{q}^{\text {col}} E_{Q | c o l}\right)\right) s e l_{i}=\left(u_{a}^{s e l}\right)^{T} \tanh \left(U_{c}^{s e l} E c o l_{i}+U_{q}^{s e l} E_{Q | c o l_{i}}\right)OP slot预测三个操作 {=,&gt;,&lt;} 计算： P_{o p}(i | Q, c o l)=\operatorname{softmax}\left(U_{1}^{o p} \tanh \left(U_{c}^{o p} E_{c o l}+U_{q}^{o p} E_{Q | c o l}\right)\right)_{i}VALUE slot.感觉”木桶效应“的短板就在这里了，此处解码器使用的是pointer network+column attention。 {P_{\mathrm{val}}(i | Q, c o l, h)=\operatorname{softmax}(a(h))} \\ {a(h)_{i}=\left(u^{\mathrm{val}}\right)^{T} \tanh \left(U_{1}^{\mathrm{val}} H_{Q}^{i}+U_{2}^{\mathrm{val}} E_{c o l}+U_{3}^{\mathrm{val}} h\right) \quad \forall i \in\{1, \ldots, L\}}$P_{\mathrm{val}}(i | Q, c o l, h)$代表着下一个token生成的概率。SQLNet简单地选择最有可能的token生成，直到生成终止符为止。 SQLovaSQLova 是韩国 Naver 提出的一种模型，全名为 Search &amp; QLova，是作者们所在部门的名称。此方案是于 SQLNet 的基础上，在模型结构方面做了一些改进而得到的，并没有提出一些创新性的解决方案。 paper：https://arxiv.org/abs/1902.01069 code：https://github.com/naver/sqlova SQLova 使用了 BERT 来作为模型的输入表达层，代替了词向量。为了让自然语言问句与表结构更好地结合，模型将自然语言问句与列名一并作为输入进行编码。同时，模型在拼接输入时采用了不同的 Segmentation Embedding 来让 BERT 可以区分问题和列名。 X-SQL X-SQL 是微软 Dynamics 365 提出一种方案。这个方案同样继承了解耦任务的思路，将预测 SQL 分解为 6 个子任务。但不同于 SQLova，X-SQL 引入了更多创新性的一些改进，主要包括以下几个方面。 首先，X-SQL 使用了 MT-DNN 来作为编码层，代替了 SQLova 中使用的 BERT，因为 MT-DNN 在很多其它自然语言处理的下游任务上取得了比 BERT 更好的效果。 其次，X-SQL 在 SQLova 中原有的 Question Segmentation 和 Column Segmentation 的基础上拓展为 Question、Categorical Column、Numerical Column 以及 Empty Column 这四种 Segmentation Embedding，分别用来作为自然语言问句、文本类型的列、数字类型的列以及空列的相应输入。通过引入更多的分类表达，可以让模型得以区分数字与文本类型的列，进而更好地生成 SQL。 最后，在输出层与损失函数部分，Where-Column 的损失函数被定义为了 KL 散度。并且，借助之前提到的引入的特殊列 [EMPTY]，如果模型在预测 Where-Column 时，分数最高的列是 [EMPTY]，那么就无视 Where-Number 所得到的预测结果，判断 SQL 语句为没有条件。 通过这些输入与结构上的优化，X-SQL 可以在 WikiSQL 的测试集上取得 86.0% 的 Logic Form 准确率和 91.8% 的 Execution 准确率。 苏剑林老师基于BERT的baseline 代码：https://github.com/bojone/bert_in_keras/blob/master/nl2sql_baseline.py 评价标准对于模型的评估，其主要有两个指标：Logic form Accuracy和Execution Accuracy，最后选手排名的依据是两项指标的平均值。 精确匹配率（Logic Form Accuracy）: 预测完全正确的SQL语句。其中，列的顺序并不影响准确率的计算。执行准确率（Execution Accuracy）: 预测的SQL的执行结果与真实SQL的执行结果一致。 计算公式如下： Score_{l f}=\left\{\begin{array}{ll}{1,} & {S Q L^{\prime}=S Q L} \\ {0,} & {S Q L^{\prime} \neq S Q L}\end{array}\right. A c c_{l f}=\frac{1}{N} \sum_{n=1}^{N} S c o r e_{l f}^{n} Score_{e x}=\left\{\begin{array}{ll}{1,} & {Y^{\prime}=Y} \\ {0,} & {Y^{\prime} \neq Y}\end{array}\right. A c c_{e x}=\frac{1}{N} \sum_{n=1}^{N} S c o r e_{e x}^{n}最后线上的评估指标为（Logic Form Accuracy + Execution Accuracy）/ 2 相似任务数据集WikiSQLWikiSQL数据集提供了8w多条有标签数据，足以满足目前的数据驱动型算法对数据量的需求。目前榜单上，效果最好的模型是SQLova和X-SQL，它们在测试集上分别可以达到89.6%和91.8%的执行准确率。 SpiderCSpider参考NL2SQL：弱监督学习与有监督学习完成进阶之路 让机器自动写SQL语言，首届中文NL2SQL挑战赛等你来战 基于Bert的NL2SQL模型：一个简明的Baseline 中文NLSQL比赛官方资料 SQLNet——知乎 一文了解Text-to-SQL How to Talk to Your Database]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Attention模型方法综述]]></title>
    <url>%2FAttention%E6%A8%A1%E5%9E%8B%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0%EF%BC%88%E5%A4%9A%E7%AF%87%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Attention背景介绍 先简单谈一谈attention模型的引入。以基于seq2seq模型的机器翻译为例，如果decoder只用encoder最后一个时刻输出的hidden state,可能会有两个问题。 encoder最后一个hidden state, 与句子末端词汇的关联较大，难以保留句子起始部分的信息； encoder按顺序依次接受输入，可以认为encoder产出的hidden state包含词序信息。所以一定程度上decoder的翻译也会按照原始句子的书序依次进行，但实际中翻译却未必如此。 为了一定程度上解决以上的问题，14年的一篇文章Sequence to Sequence Learning with Neural Network提出了一个有意思的trick，即在模型训练的过程中将原始句子进行反转，取得了一定的效果。 Attention的计算在计算Attention时主要分三步： query和每个key进行相似度计算得到权重，常用的相似度函数有点积、拼接和感知机等； 使用softmax函数对这些权重进行归一化； 将权重和相应的键值进行加权求和得到最后的attention。 f(Q, K_j) = \begin{cases} Q^T K_j, & \text {dot} \\ Q^T W_a K_j, & \text {general} \\ W_a[Q,K_j], & \text concat \\v_a^Ttanh(W_a[Q;K_j]) & \text {perceptron} \end{cases} \alpha_j = softmax(f(Q,K_j)) = \frac {exp(f(Q,K_j))}{\sum_j exp(f(Q, K_j))} Attention(Q,K,V) = \sum_j \alpha_j V_j相关论文Attention机制的本质来自于人类视觉注意力机制。 Show, Attend and Tell（Soft/Hard Attention）■ 论文 | Effective Approaches to Attention-based Neural Machine Translation ■ 源码 | https://github.com/kelvinxu/arctic-captions 文章中提出了两种attention模式，即hard attention和soft attention。 这是对attention的另一种分类。Soft attention本质上和Bahdanau提出的很相似，其权重取值在0到1之间，而Hard Attention取值0或者1。hard attention专注于很小的区域，而soft attention的注意力相对发散。 Attention-based NMT ■ 论文 | Effective Approaches to Attention-based Neural Machine Translation ■ 源码 | https://github.com/lmthang/nmt.matlab 这篇论文是继上一篇论文之后，一篇很具代表性的论文。他们的工作告诉了大家attention在RNN中可以如何进行扩展，这篇论文对后续各种基于attention的模型在NLP的应用起到很大的促进作用。在论文中她们提出了两种attention机制，一种是全局（global）机制，一种是局部（local）机制。 在decoder的第t时刻，利用global attention或local attention得到context vector($c_t$)之后，对$h_t$和$c_t$进行concatenate操作得到attention hidden state。 \tilde{h}_t = tanh(W_c[c_t;h_t])最后利用softmax产出该时刻的输出： p(y_t|y_{]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于 Hexo + Github Pages 搭建个人主页]]></title>
    <url>%2F%E5%9F%BA%E4%BA%8E%20Hexo%20%2B%20Github%20Pages%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[准备知识什么是Github Pages?什么是Hexo？Hexo是一个快速、简洁且高效的博客框架。Hexo使用Markdown解析文章，在几秒内，即可使用靓丽的主题生成静态网页。 搭建步骤环境 Ubuntu 16.04 git Node.js：建议使用nvm进行安装 123wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.34.0/install.sh | bashnvm install stable 安装Hexo官方安装文档 1$ npm install hexo-cli -g 在本地第一次部署1234$ hexo init blog$ cd blog$ hexo generate$ hexo server 现在在浏览器中输入localhost:4000 public: 执行hexo generate命令，输出的静态网页内容目录 scaffolds: layout模板文件目录 scripts： 扩展脚本目录，这里可以自定义一些javascript脚本 source： 文章源码目录，该目录下的markdown和html文件均会被hexo处理 drafts: 保存草稿文章 posts: _config.yml: 网站的全局配置文件 package.json: hexo的版本信息 使用Next主题在Hexo中有两份主要的配置文件，其名称都是_config.yml。其中一份位于站点根目录下，主要包含Hexo本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。为了区分，我们将前者称为站点配置文件，后者称为主题配置文件。 下载Next主题1234$ git clone https://github.com/iissnan/hexo-theme-next themes/next``` 修改主目录下的_config.yml配置文件 theme: next12在本地测试网站 $ hexo clean$ hexo generate$ hexo server``` 参考Github Pages+Hexo搭建个人博客]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《A Simple Theoretical Model of Importance for Summarization》阅读笔记]]></title>
    <url>%2F%E3%80%8AA-Simple-Theoretical-Model-of-Importance-for-Summarization%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文题目：（Long）A Simple Theoretical Model of Importance for Summarization 论文作者：Maxime Peyrard 论文链接：https://www.aclweb.org/anthology/P19-1101.pdf ACL 2019 文本摘要是一种基于背景知识的有损语义压缩过程，其核心问题是如何定位和筛选重要信息（Importance Information）。除了直接提供文本摘要结果供用户阅读外，也为很多其他下游任务（长文本情感分析、搜索引擎、推荐系统等）提供辅助。 本论文是来自瑞士洛桑联邦理工学院Maxime Peyrard的工作，ACL 2019杰出论文【outstanding paper(5)】。 作者以往的相关工作： Studying Summarization Evaluation Metrics in the Appropriate Scoring Range, ACL 2019(Short) A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments, ACL 2017 Optimizing an Approximation of ROUGE - a Problem-Reduction Approach to Extractive Multi-Document Summarization, ACL 2016 本文不涉及复杂模型和庞大的训练语料，其以信息论)为基础，来探究什么样的摘要是一个好摘要。为了进一步提升摘要的质量，本文提出了几个概念来指导摘要生成的过程： 冗余度（Redundancy） 相关性（Relevance） 信息量（Informativeness） 论文理论的依据和前提本文采用的是香农信息论（香农的信息论究竟牛在哪里？）中的基本理论。摘要的主要目的就是在损失最小信息量的情况下，最大限度表达原文信息量。论文将文本切分成最基本的语义单元 $\omega_{i}$，语义单元负责语义部分，而信息论只需要关注由语义单元构成的文本信息即可。 H(X)=-\sum_{\omega_{i}} \mathbb{P}_{X}\left(\omega_{i}\right) \cdot \log \left(\mathbb{P}_{X}\left(\omega_{i}\right)\right)一下介绍中引入的部分术语符号如下： $\omega_{i}:$ 表示一个语义单元（字符，词，n-gram或者具有更复杂语义语法内容的单元） $\Omega:$ 表示由语义单元 $\omega_{i}$ 组成的集合 $X$: 一段自然语言的文本 $\mathbb{P}_{X}\left(\omega_{i}\right)$: 表示文本 $X$ 包含原子信息 $\omega_{i}$ 的概率（类似语言模型的感觉） 度量的几个维度冗余度（Redundancy） 当处理较长的文本是，只以相关性为目标做摘要会遇到一个很明显的问题：模型倾向于生成多个相似度很高的摘要，而丢失了一些小众主题的信息。以往，也有很多工作对摘要的冗余度进行研究。 MMR（Maximal Marginal Relevance）：将相关性和冗余度放在一个目标函数中，使用贪心算法优化目标函数。每次挑选新的摘要句 $d_i \in D$ 时，除了 1）建模其与相关性分数外，还要 2）扣除其与当前摘要集合 $R$ 的冗余度分数，最后 3）挑选综合分数最高的候选句加入到摘要集合中。 \operatorname{MMR}(\mathrm{D}, \mathrm{D}, \mathrm{R})=\operatorname{Arg} \max _{\mathrm{d}_\mathrm{i} \in \mathrm{D}}^{k} \left[\underbrace{\lambda \operatorname{sim}\left(\mathrm{D}, \mathrm{d}_{\mathrm{i}}\right)}_{Relevance} - \underbrace{(1-\lambda) \max _{\mathrm{d}_{\mathrm{j}} \in \mathrm{R}}\left(\operatorname{sim}\left(\mathrm{d}_{\mathrm{i}}, \mathrm{d}_{\mathrm{j}}\right)\right)}_{Redundancy}\right] Submodular。submodular在建模冗余度时，不是惩罚冗余性，而是奖励多样性。 文中直接使用信息论中的熵来度量摘要的信息量，对冗余度进行建模。 \operatorname{Red}(S)=H_{\max }-H(S)其中 $H_{\max }=\log |\Omega|$是独立于摘要 $S$ 的约束条件，因此公式可以简化为如下的形式： \begin{eqnarray*} \operatorname{Red}(S)&=&-H(S) \\ \\ &=& \sum_{\omega_{i}} \mathbb{P}_{S}\left(\omega_{i}\right) \cdot \log \left(\mathbb{P}_{D}\left(\omega_{i}\right)\right) \end{eqnarray*} 注：熵越大，文本的不确定性越高，信息量也越大，那么其冗余度也越小。 相关性（Relevance） 相关性是摘要生成问题中最基本的一个要求，大多数模型对摘要抽取或生成的目标都可以近似为相关性。以下为一些基于相关性对摘要进行建模的方法： 基于关键词的方法：先使用关键词抽取模型抽取关键词，然后统计包含关键词最多的句子作为候选摘要。关键词抽取应用比较广泛的就是基于tfidf方法。 基于主题模型：如LDA，LSA等，分析文档隐含的主题，然后分析句子和主题的相关性。 将句子向量化表示：然后对句子进行聚类，隐含的每个聚类代表某个主题，然后从这些主题中挑选摘要句。 graph-based方法：以textrank为经典方法，将句子作为节点，句子之间的相似度关系作为边，构建有权图，利用图论中的算法，得到每个句子的权重分数。 这种方法，相比较于前面三个不太直观，实质上，它挑选的句子通常是相似性最强的一堆句子中的一个。即textrank认为一个句子如果与它相似的句子数越多，表明这个句子与文档主题内容越相关。 本文用交叉熵来建模摘要与原文的相关性： \begin{eqnarray*} \operatorname{Rel}(S, D)&=&-C E(S, D) \\ \\ &=& \sum_{\omega_{i}} \mathbb{P}_{S}\left(\omega_{i}\right) \cdot \log \left(\mathbb{P}_{D}\left(\omega_{i}\right)\right) \end{eqnarray*} 注：交叉熵越小，表示摘要和文档的差异越小，那么相关性越强。 整合相关性和冗余度 KL散度很好地整合了相关性和冗余度。当$S$与 $D$的KL散度很小的时候，说明摘要拟合原文的效果非常好，此时相关性和冗余度的综合分数就比较高。 \begin{eqnarray*} \operatorname{Rel}(S, D) - Red(S) &=&-C E(S, D) + H(S) \\ \\ &=& - KL(S||D) \end{eqnarray*}“信息量”（Informativeness） 根据论文所说，假设当前有一个背景知识库 $K$ ，从文档 $D$ 中得到的摘要 $S$ 对于$K$来说应尽可能增多信息。这样才能使读者在阅读摘要后获取更多的信息。 \begin{eqnarray*} \operatorname{Inf}(S, K)&=& CE(S, K) \\ \\ &=& -\sum_{\omega_{i}} \mathbb{P}_{S}\left(\omega_{i}\right) \cdot \log \left(\mathbb{P}_{K}\left(\omega_{i}\right)\right) \end{eqnarray*}protential Information 本概念的目标主要是为了对informativeness建模提供上界。简单来说，在已知背景知识 $K$ 的条件下，我们也能从 $D$ 中获取新的信息。同样也可以使用交叉熵来建模两个分布的差异，公式如下： \begin{eqnarray*} \operatorname{PI_K}(D, K)&=& CE(D, K) \\ \\ &=& -\sum_{\omega_{i}} \mathbb{P}_{D}\left(\omega_{i}\right) \cdot \log \left(\mathbb{P}_{K}\left(\omega_{i}\right)\right) \end{eqnarray*}重要性（Importance） = Relevance + InformativenessImportance是论文提出的一个新的概念，它针对的是语义单元，目标是计算每个语义单元的重要性分数。在构造摘要的时候，会根据这个分数判断是否选择某个语义单元。而且产生摘要应仅通过使用$D$中可用的信息，来使知识背景为$K$的用户带来最新的信息。构造这样一个打分函数 $f\left(d_{i}, k_{i}\right)$，需要满足以下约束条件： Informativeness: 不同的语义单元 $\omega_{i}, \omega_{j}, i \neq j$ 在文档中的概率分布相同 $d_i = d_j$且$k_i &gt; k_j$，则$f\left(d_{i}, k_{i}\right)&lt;f\left(d_{j}, k_{j}\right)$，语义单元的重要性分数$f$ 与informativeness负相关。 Relevance: 不同的语义单元 $\omega_{i}, \omega_{j}, i \neq j$ 在 $K$ 中的概率分布相同 $d_i &gt; d_j$且$k_i = k_j$，则$f\left(d_{i}, k_{i}\right) &gt; f\left(d_{j}, k_{j}\right)$，语义单元的重要性分数$f$ 与informativeness负相关。 Additivity: 打分函数应当保持信息论中度量方法的加性性质 $I\left(f\left(d_{i}, k_{i}\right)\right) \equiv \alpha I\left(d_{i}\right)+\beta I\left(k_{i}\right)$ Normalization: $\sum_{i} f\left(d_{i}, k_{i}\right)=1$ 为了满足上述四个条件，论文给出一个函数实例： \begin{aligned} \mathbb{P}_{\frac{D}{K}}\left(\omega_{i}\right) &=\frac{1}{C} \cdot \frac{d_{i}^{\alpha}}{k_{i}^{\beta}} \\ C &=\sum_{i} \frac{d_{i}^{\alpha}}{k_{i}^{\beta}}, \alpha, \beta \in \mathbb{R}^{+} \end{aligned} 注：$d_{i}=\mathbb{P}_{D}\left(\omega_{i}\right)$, $k_{i}=\mathbb{P}_{K}\left(\omega_{i}\right)$ 对于重要性（Importance）来说，它整合了Relevance和Informativeness两个维度，由此得到的理想摘要应当能尽可能拟合概率分布 $\mathbb{P}_{\frac{D}{K}}$ , 这里仍用交叉熵来度量分布的差异，因此公式如下： \text { Importance }\left(S, \frac{D}{K}\right)=-C E\left(S, \frac{D}{K}\right)整合各个维度上文所述的重要性已经整合了Relevance和Infornativeness两个维度，要得到最终的优化目标，就需要把Redundancy也整合进来。 \begin{aligned} \theta_{I}(S, D, K) &= \underbrace{\alpha Rel(S, D) + \beta Inf(S, K)}_{Importance} - Red(S) \\ &=-C E\left(S, \frac{D}{K}\right) + H(S)\\ &=-K L\left(\mathbb{P}_{S} \| \mathbb{P}_{\frac{D}{K}}\right) \end{aligned}实验论文并没有使用一些传统的Rouge、Bleu等评测方法对摘要进行评测，而是使用金字塔的计算方式对机器和人工生成的摘要各自进行评分。最后计算两者的相关系数，以评测机器生成的摘要是否达到了人工的水平。 参考抛开模型，探究文本自动摘要的本质——ACL2019 论文佳作研读系列 ACL 2019 A Simple Theoretical Model of Importance for Summarization]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>text summarization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《基于主题模板和结构化卷积解码器生成摘要》阅读笔记]]></title>
    <url>%2F%E3%80%8A%E5%9F%BA%E4%BA%8E%E4%B8%BB%E9%A2%98%E6%A8%A1%E6%9D%BF%E5%92%8C%E7%BB%93%E6%9E%84%E5%8C%96%E5%8D%B7%E7%A7%AF%E8%A7%A3%E7%A0%81%E5%99%A8%E7%94%9F%E6%88%90%E6%91%98%E8%A6%81%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[来源：ACL 2019 原文链接：https://www.aclweb.org/anthology/P19-1504 代码链接：https://github.com/lauhaide/WikiCatSum Introduction本文基于假设——“将内容结构考虑在内的解码器会使摘要生成的结果更好，同时也会减少通用回复的问题”， 进行了以下研究。虽然文中多次提到多文档摘要，但并不是处理真正意义上的多文档摘要问题。 Model本文模型仍然遵循传统的编码器-解码器架构，将输入文本编码成隐向量后从中解码出最终的摘要文本。模型的总体架构如下： CNN Encoder该模型在使用CNN [Gehring et al., 2017] 将输入文本编码，(作者说这里使用CNN来进行编码的原因是1）有利于并行训练；2）在生成式摘要任务上表现优秀。但后面解码时用的是LSTM，因此并行的效果有待考证) Hierarchical Convolutional Decoder预测输出$y_{ti}$是通过卷积网络最顶层的输出$P\left(y_{t i} | y_{t\{1 : i-1\}}\right)=\operatorname{softmax}\left(W_{y}\left(\mathbf{o}_{t i}^{L}+\mathbf{c}_{t i}^{L}\right)\right)$。决定的该模型通过优化负对数似然$\mathcal{L}_{N L L}$来进行训练 Document-level Decoder在该层会解码出句子表示序列$\left(\mathbf{s}_{1}, \cdots, \mathbf{s}_{|\mathcal{S}|}\right)$。(但是这里并不清楚$s_t$解码停止条件) $\mathbf{h}_{t}=\mathrm{LSTM}\left(\mathbf{h}_{t-1}, \mathbf{s}_{t-1}\right)$ $\mathbf{s}_{t}=\tanh \left(\mathbf{W}_{s}\left[\mathbf{h}_{t} ; \mathbf{c}_{t}^{s}\right]\right)$ 这里使用了soft attention [Luong et al., 2015] 的机制，详细原理可以参考这篇博客)。 $\alpha_{t j}^{s}=\frac{\exp \left(\mathbf{h}_{t} \cdot \mathbf{z}_{j}\right)}{\sum_{j^{\prime}} \exp \left(\mathbf{h}_{t} \cdot \mathbf{z}_{j^{\prime}}\right)}$ $\mathbf{c}_{t}^{s}=\sum_{j=1}^{|\mathcal{X}|} \alpha_{t j}^{s} \mathbf{z}_{j}$ Sentence-level Decoder在该部分，作者定义了target摘要句的词表示由 a）词嵌入；b）句内位置向量；c）句子位置向量，三部分组成。 $\mathbf{g}_{t i}=\operatorname{emb}\left(y_{t i}\right)+\mathbf{e}_{i}+\mathbf{e}_{t}$ 每个摘要句子$s_{t}=\left(y_{t 1}, \dots, y_{t\left|s_{t}\right|}\right)$都是由sentence-level decoder生成的。 \left\{\mathbf{o}_{t 1}^{l}, \cdots, \mathbf{o}_{t n}^{l}\right\}=\operatorname{conv}\left(\left\{\mathbf{o}_{t 1}^{\prime l-1}, \cdots, \mathbf{o}_{t n}^{\prime l-1}\right)\right. \mathbf{d}_{t i}^{l}=W_{d}^{l}\left(\mathbf{o}_{t i}^{l}+\mathbf{s}_{t}\right)+\mathbf{g}_{t i} a_{t i j}^{l}=\frac{\exp \left(\mathbf{d}_{t i}^{l} \cdot \mathbf{z}_{j}\right)}{\sum_{j^{\prime}} \exp \left(\mathbf{d}_{t i}^{l} \cdot \mathbf{z}_{j^{\prime}}\right)} \mathbf{c}_{t i}^{l}=\sum_{j=1}^{|\mathcal{X}|} a_{t i j}^{l}\left(\mathbf{z}_{j}+\mathbf{e}_{j}\right) \mathbf{o}_{t i}^{\prime l}=\mathbf{o}_{t i}^{l}+\mathbf{s}_{t}+\mathbf{c}_{t i}^{l}Topic Guidance在主题判别模块，本文将每个句子看做一个文档，并利用LDA模型分析其中隐含的主题列表K，并为每一个句子打上最可能的主题标签。目的是是document-level decoder更能与主题相关。在这里，作者设计了一个辅助任务用document-level decoder得到的隐层表示来预测主题$k_{t}$，$P\left(k_{t} | s_{1 : t-1}\right)=\operatorname{softmax}\left(W_{k}\left(\mathbf{s}_{t}\right)\right)$ （这里不太清楚为什么不是$P(k_{t} | s_{t})$ ）。 符号说明 $\alpha_{t j}^{s}$：sentence-level decoder中attention的权重 $a_{t i j}^{l}$：document-level decoder中attention的权重 ${c}_{t}^{s}$：document-level decoder中得到的attention表示 $e_i$：句内位置向量 $e_t$：句子位置向量 $\mathbf{o}_{t i}^{l}$：卷积神经网络第l层对第t句摘要的第i个词的解码输出 $s_t$：document-level decoder解码出第t句的隐层状态表示 $y_{t i}$：输出摘要中第t句的第i个词 Experiments本文在自己构造的数据集WIKICATSUM上进行实验，具体的数据集参数如下： 实验中将模型与谷歌2018年的工作进行对比。结果如下（TF-S2S是谷歌的工作） Conclusions（个人观点）本文最大贡献是分层解码部分，document-level decoder部分利用辅助任务捕捉到topic信息。 参考Liu P J, Saleh M, Pot E, et al. Generating wikipedia by summarizing long sequences[J]. ICLR 2018. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. 2017. Convolutional Sequence to Sequence Learning ICML 2017 Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. EMNLP 2015 ACL 2019 | 利用主题模板进行维基百科摘要生成]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《Evolving Dialogue Strategy via Compound Assessment》阅读笔记]]></title>
    <url>%2F%E3%80%8AEvolving-Dialogue-Strategy-via-Compound-Assessment%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文题目：Know More about Each Other: Evolving Dialogue Strategy via Compound Assessment 论文链接：https://arxiv.org/pdf/1906.00549.pdf ACL 2019 Introduction本文认为在人类对话中，最终目标之一是信息可以通过互动有效地交换。 特别是，我们认为成功的多轮对话是由对话中两个参与者的共同经验决定的，即两个参与者都需要了解他们的对应部分并有效地表达自己。为此，我们提出让双方更多了解彼此的目标。为此目的，引入了一个新的生成 - 评估框架，用于多轮对话。 本文的主要贡献 提出了一个新颖的生成-评估框架，这样有助于产生信息和连贯的对话。 为了评估对话策略的有效性，本文专门针对了信息量和连贯性设计了两个指标，这些指标进一步整合为复合奖励。为了最大化这种奖励，通过强化学习的方法加强知识选择策略。 Model 本文的生成-评估框架如上图所示，根据知识选择策略，两个对话代理根据相应的背景交替介绍自己。通过信息丰富性和相关性两个方面协同评估对话策略。 Dialogue Generation以对话历史和背景知识作为输入，对话策略选择一条适当的知识来产生信息丰富且流畅的响应。背景$\mathcal{Z}=\left\{z_{1}, z_{2}, \cdots, z_{M}\right\}$包含一组知识的集合，在这里知识以一句话描述的形式表现，例如I like to ski。Utterance $u_{t-1}$是另一个参与者的最后一句响应，context $c_t=concat(u_{1}, u_{2}, \cdots, u_{t-1})$为当前对话历史信息。 在对话中，输入的对话上下文历史信息$c_t$分为两部分：$u_{t-1}$和$c_{t-1}$，分别用独立的两个编码器进行编码。为了回复的连贯性，在t回合使用的知识应该在语义上与伙伴的最后一个话语$u_{t-1}$相关；为了避免重复，第t回合使用的知识应与之前对话历史$c_{t-1}$不同。 得到对话和知识信息的表示 knowledge——$z_{i}^{G}$，utterance——$u_{t-1}^{G}$，context——$c_{t-1}^{G}$，（$\mathcal{Z}^{G}=\left\{z_{1}^{G}, z_{2}^{G}, \cdots, z_{M}^{G}\right\}$）。知识的先验分布$p\left(\mathcal{Z} | c_{t}\right)$可以通过MLP attention来进行预测 {p\left(\mathcal{Z} | c_{t}\right)=p\left(\mathcal{Z} | u_{t-1}\right) * 0.5+p\left(\mathcal{Z} | c_{t-1}\right) * 0.5} {p\left(z_{i} | u_{t-1}\right)=\operatorname{softmax}\left(\operatorname{MLP}-\operatorname{ATT}\left(u_{t-1}^{G}, z_{i}^{G}\right)\right)} {p\left(z_{i} | c_{t-1}\right)=\operatorname{softmax}\left(\operatorname{MLP}-\mathrm{ATT}\left(c_{t-1}^{G}, z_{i}^{G}\right)\right)}MLP-ATT的计算公式如下： \operatorname{MLP}-\operatorname{ATT}(x, y)=V_{1}^{T} \tanh \left(x W_{1}+y W_{2}\right)注：$W_{1}, W_{2} \in \mathbb{R}^{d \times d}$，$V_{1} \in \mathbb{R}^{d}$，$p\left(\mathcal{Z} | c_{t}\right)$是知识选择的概率分布，$\sum_{i=1}^{M} p\left(z_{i} | c_{t}\right)=1$。根据概率分布$p\left(\mathcal{Z} | c_{t}\right)$来选择所需的知识$z_i$，并将其送入解码器生成回复的概率$p\left(u_{t} | z_{i}, u_{t-1}\right)$。 显然，对于信息丰富和连贯的对话最关键的部分是知识选择。然而，高精度的解码$p\left(u_{t} | z_{i}, u_{t-1}\right)$也同样重要。这部分通过已有的标准回复进行有监督的预训练实现，训练数据的格式为$\left\{u_{t-1}, z_{i}, u_{t}\right\}$。预训练的主要步骤如下： 1）将知识和utterance编码为$z_{i}^{G}$和$u_{t-1}^{G}$ 2）基于标准知识和最后一个的utterance（ $u_{t-1}$ ）生成回复$u_t$ 3）通过监督学习优化编码器和解码器的参数 Strategy Evaluation本文中的的对话是由两个对话代理生成（agent）。为了评估部署策略的有效性，首先收集生成的对话文本和对话代理的背景知识，然后精心设计指标评估对话的信息性（informativeness）和一致性（coherence）。 informativeness信息是产生有意义对话的关键因素，但由于缺乏对有效信息利用的彻底控制，很容易产生重复的话语。本文设计了一种新颖的信息量度量的指标来衡量对话层面信息的有效利用。 coherence \alpha_t = sigmoid(v_u^T s_t) P_g(y_t=w_g) = softmax(W_g^o s_t) P_e(y_t=w_e) = softmax(W_e^o s_t) y_t o_t =P(y_t) = \alpha_t P_e(y_t=w_e) + (1-\alpha_t) P_y(y_t=w_g)数据集-]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>dialogue system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeeCamp 2019：Day 1]]></title>
    <url>%2FDeeCamp-2019%EF%BC%9ADay-1%2F</url>
    <content type="text"><![CDATA[7月20日，DeeCamp 2019正式开营。 创新工场董事长兼CEO李开复，上海交通大学特聘教授、博士生导师俞勇，香港科技大学计算机系和数学系，机器学习领域的世界级专家张潼，人工智能领域世界级专家、南京大学人工智能学院院长周志华四位重磅级导师亲临北京、上海、广州、南京四座城市，分别开讲第一课。 以下是今天四位老师分享的课题： AI应用阶段的全球人才挑战——李开复 AI 教育与教育AI：愿景、技术和挑战——俞勇 机器学习的一些前沿方向和进展——张潼 机器学习现阶段的挑战——周志华 机器学习的一些前沿方向 前沿 更深更复杂的模型提升效果——&gt;基于向量的表示获得广泛应用——&gt;自动化机器学习——&gt;基于模拟的强化学习 复杂场景、小数据学习、物理世界中具体任务 复杂模型Why deep NN? More efficient representation of some complex functions. Over parameterized DNN can be efficient learned（high complexity matters：Adding layers is more effective than adding units） big training data powerful computational facilities training tricks（dropout、batch normalization、residual networks等） Deep model的关键 逐层处理 layer-by-layer processing 内置特征变换 模型复杂度足够 现实世界并非所有规律性质都是可微的，或者通过可微构件建模最优。在图像、视频、语音之外的很多任务上，深度神经网络并非最佳选择不少时候甚至表现不佳。例如，在很多kaggle competition任务上，随机森林或者是XGBoost更好 决策树？Boosting? 决策树和Boosting虽然可以逐层处理，但是复杂度不够并且没有特征变换。 an Alternative to Deep Neural Networks: dcforest 表示学习目标：represent data by a vector, also called embedding. The vector can be used for different machine learning tasks. 什么是表示学习？ 通俗易懂的解释就是用数字（向量、矩阵…）来表达现实世界中的物体（object），而且这种表达方式有利于后续的分类或者其他决策问题。 对于NLP方向的表示学习可以参考下面这篇博客——《NLP文本表示预训练技术》) 自动机器学习Components of an NAS System Search Space Design Search Algorithm Design Reinforcement Learning Evolutionary Algorithms Continuous relaxation 机器学习现阶段的挑战一些问题 鲁棒性不强 场景变动带来的自适应性不强 任务可扩展性不强 对世界知识表示得不好 能否基于不可微的构件进行深度学习？ unkown unkowns 我不知道我不知道这件事 robust AI 弱监督学习 任务环境静态 数据分布恒定 样本类别恒定 属性恒定 优化目标恒定 How to handle emerging new class in stream？ 增量学习（Incremental learning） 机器学习+逻辑系统 inductive 附一张开营照，以证清白 参考深度学习的未来：神经网络架构搜索(NAS) 神经网络架构搜索（NAS）综述 | 附AutoML资料推荐 Advanced Machine Learning Day 3: Neural Architecture Search Liang S , Srikant R . Why Deep Neural Networks for Function Approximation? ICLR-17 Mhaskar H, Liao Q, Poggio T. When and why are deep networks better than shallow ones?[C]//Thirty-First AAAI Conference on Artificial Intelligence. 2017. Perekrestenko D, Grohs P, Elbrächter D, et al. The universal approximation power of finite-width deep ReLU networks[J]. arXiv preprint arXiv:1806.01528, 2018. Du S S, Zhai X, Poczos B, et al. Gradient descent provably optimizes over-parameterized neural networks[J]. arXiv preprint arXiv:1810.02054, 2018. Mei S, Montanari A, Nguyen P M. A mean field view of the landscape of two-layer neural networks[J]. Proceedings of the National Academy of Sciences, 2018, 115(33): E7665-E7671.]]></content>
      <categories>
        <category>note</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[小冰架构学习]]></title>
    <url>%2F%E5%B0%8F%E5%86%B0%E6%9E%B6%E6%9E%84%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[设计思想如何评价小冰Open Domain Social Long-term Dialogue System的性能？ 单次对话论述 Conversation-turns Per Session(CPS)作为评估社交聊天机器人成功的指标。CPS越大，社交聊天机器人的对话参与能力就越好。第六代小冰的CPS已经从第一代的5提升到23。 IQ+EQ+Personality 社交聊天机器人需要足够高的智商（IQ）来学习多种技能，才能紧跟用户需求，帮助他们完成指定任务。同时，社交聊天机器人还需要足够高的情商（EQ），以满足用户情感的需求，比如情绪感受和社会归属感。 IQ：IQ能力包括知识和记忆建模、图像和自然语言理解、推理生成和预测。为了满足用户的特定需求以及帮助用户完成指定的任务，这些能力是不可或缺的。其中最重要且最复杂的技能是核心聊天（Core Chat），即与用户在多个主题上展开长时间和开放域的对话 EQ： Personality：符合小冰既有的风格 将社交聊天视为分层决策 对话可被视为有自然层级的决策过程：一个顶级过程管理者整体的对话并选取不同的技能来处理不同类型 的对话模式（比如闲聊、问答、订票）；低级过程则受所选择的技能控制。可选择基本动作（响应），从而生成对话段落或完成任务。 系统架构 上图为小冰的整体架构，其包含三层：用户体验层、对话引擎层和数据层。 用户体验层对话引擎层对话引擎层中的四个主要组件：对话管理、共情计算、核心聊天和技能。 对话管理对话管理除了记录对话历史还要包括策略管理，即管理什么时候触发Skill什么时候切换。同时，Conversation还受到Topic的管理。在Pretrain的阶段得到Topic Index，当触发Topic切换的标志时，例如： Core Chat未能生成有效的候选集 生成的响应知识用户输入的重复 用户输入变得平淡 则调用Topic切换，切换之后的Topic根据以下几个指标选取： 上下文关联性 新鲜度 个人兴趣 热度 接受度 共情计算（Empathetic Computing）共情度计算是小冰相对独特的一点，它不是直接把Context和Reply进行匹配得到一个Match Score。而是由Query, Context, Reply及情景分析得到一个$s=\left(Q_{c}, C, e_{Q}, e_{R}\right)$向量，再由向量$s$计算出候选回复。 Contextual Query Understanding 本模块的主要任务是对Context进行补全，通过命名实体识别和指代消解的方法，对对话历史信息进行补全。 User Understand Interpersonal Response Generation 核心聊天（Core chat）核心聊天是小冰的智商和情商的重要组成部分。小冰在处理语言生成的时候使用两阶段法，先构造Reply候选集，然后通过计算每个Reply的得分（Score）选出最优答案。 Retrieval-Based Generator Using Paired Data Neural Generate Generator 单纯靠检索来获得数据，会漏掉一些新的热点，覆盖面不高。在小冰的架构中，对前面构造的向量$s$做sigmoid操作$v=\sigma\left(W_{Q}^{T} e_{Q}+W_{R}^{T} e_{R}\right)$，每一轮都喂到网络中这样保证了生成的Response带有小冰的人格。 Retrieval-Based Generator Using Unpaired Data Response Candidate Ranker 数据层 参考直男届的杀手-『小冰』架构解析 沈向洋等人论文详解微软小冰，公开研发细节 The Design and Implementation of XiaoIce, an Empathetic Social Chatbot. Li Zhou et al. 18.12]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>dialogue system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《使用执行指导文本到SQL解码生成》阅读笔记]]></title>
    <url>%2F%E3%80%8A%E4%BD%BF%E7%94%A8%E6%89%A7%E8%A1%8C%E6%8C%87%E5%AF%BC%E6%96%87%E6%9C%AC%E5%88%B0SQL%E8%A7%A3%E7%A0%81%E7%94%9F%E6%88%90%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文题目：Robust Text-to-SQL Generation with Execution-Guided Decoding Execution-Guided Decoder在适当的时间步长评估部分生成的SQL语句，然后将那些无法完成的候选项排除（红色背景）。下图中，“opponent &gt; Haugar”会产生runtime error，而“opponent = UEFA”会产生一个空的查询。 Execution-Guided Decoding这项工作的关键是，部分生成的SQL语句，可以为生成过程的其余部分提供指导。这项工作仅使用此信息来过滤掉无法完成正确查询的部分结果。 执行错误(Execution Errors)SQL语句执行引擎可以识别以下问题： Parsing errors：如果程序在语法上不正确，则会导致解析错误。这种错误在复杂查询中更常见（如GeoQuery和ATIS数据集中出现的那样）。与基于模板和基于插槽填充的模型相比，自回归模型更容易出现此类错误。 Runtime erros：如果程序p的运算符类型与其操作数类型不匹配，则程序p会抛出运行时错误。这样的错误可能是由于聚合函数与其目标列之间的不匹配（例如，对具有字符串类型的列的总和）或条件运算符与其操作数之间的不匹配（例如，应用&gt;到一个float类型的列和一个string类型的常量）。 Empty output：如果解码器生成的限制过于严格，那么程序有可能返回空结果 用执行结果指导解码（Using Execution Guidance in Decoding）EG + autoregressive decoder它可以被视为应用于模型特定解码器单元DECODE的标准波束搜索的扩展。只要有可能（即，当当前时间步t的结果对应于可执行的部分程序时），该过程仅保留波束中对应于部分程序的前k个状态而没有执行错误或空输出。 EG + non autoregressive decoder在基于前馈网络的非自回归模型中，execution guidance可以用在解码后的过滤部分。（例如，舍弃产生execution errors的结果） 实验Pointer-SQL+EG Coarse2Fine+EGCase Study参考Execution-Guided Decoding——知乎]]></content>
      <categories>
        <category>NLP</category>
        <category>NLG</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[知识驱动的对话生成]]></title>
    <url>%2F%E7%9F%A5%E8%AF%86%E9%A9%B1%E5%8A%A8%E5%AF%B9%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[该任务是基于2019年语言与智能竞赛，知识驱动对话赛道。当前聊天机器人不够主动，并且回复信息不够丰富。 任务给定目标 $g$ 及相关知识信息 $M=f_1, f_2, …, f_n$ ，要求参评的对话系统输出适用于当前新对话序列 $H=u_1, u_2,…,u_{t-1}$ 的机器回复$u_t$使得对话自然流畅、信息丰富而且符合对话目标的规划。在对话过程中，机器处于主动状态，引导用户从一个话题聊到另一个话题。因此，对话系统为机器设定了一个对话目标，$g$为“START-&gt;TOPIC_A-&gt;TOPIC_B”，表示从冷启动状态主动聊到话题A，然后进一步聊到话题B。 比赛数据数据中的知识信息来源于电影和娱乐人物领域有价值的信息，如票房、导演、评价等，以三元组SPO的形式组织。对话目标中的话题为电影或娱乐人物实体，数据集中共有3万session，大约12万轮对话，其中10万训练集，1万开发集，1万测试集。 基线模型该模型主要由四部分组成，分别为对话内容编码器、知识编码器、知识管理器和解码器。 paper: https://arxiv.org/abs/1902.04911 code: https://github.com/baidu/knowledge-driven-dialogue Encoder在这部分，模型使用双向GRU来对对话内容和知识进行编码。我们定义$X$为多轮对话内容，$K$为知识库信息，$Y$为真实的回复。编码器将对话内容 $X$ 和知识信息 $K$ 编码成向量 $x$ 和 $k$，之后送入Knowledge manager。编码对话内容和知识信息的两个编码器遵循同样的结构，但不共享参数。 h_t =[\overrightarrow{h}_t ; \overleftarrow{h}_t]=[GRU(x_t, \overrightarrow{h}_{t-1});GRU(x_t, \overleftarrow{h}_{t+1})]Knowledge Manager知识管理模块这部分的作用主要为从外部知识中选出所需知识。在训练过程中，X和Y均被模型作为输入进行训练。这是通过后验信息得到知识的概率分布，计算公式如下： p(k=k_i |x,y)=\frac{exp(k_i \cdot MLP([x;y]))}{\sum_{j=1}^{N} exp(ki \cdot MLP([x;y]))}但在生成预测回复的时候，$Y$对于我们来说是未知的，所以我们只能通过输入$X$来进行知识选择。具体公式如下： p(k=k_i|x)=\frac{exp(k_i \cdot x)}{\sum_{j=1}^N exp(k_j \cdot x)}在训练和预测回复的过程中，选择知识$k$分别是依据后验概率$p(k|x,y)$和$p(k|x)$。因此知识管理模块采用KLDivLoss（KullbackLeibler divergence loss）作为损失函数来衡量先验和后验的相似性。 \mathcal{L}_{KL}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}p(k=k_i|x,y)log\frac{p(k=k_i|x,y)}{p(k=k_i|x)}Knowledge ManagerDecoder在上下文内容$c_t$和所选择的$k_i$的条件下，模型的解码器按顺序生成回复。与传统的Seq2Seq解码器不同，该模型将知识融入到回复生成中。因此，模型中介绍两种解码器。 Standard GRU with Concatenated Inputs $s_{t-1}$是GRU的上一步隐层状态，$c_t$是基于attention的上下文向量。 s_t = GRU([y_{t-1};k_i], s_{t-1}, c_t)Hierarchical Gated Fusion Unit HGFU(Hierarchical Gated Fusion Unit)的中文是分级门控融合单元，其提供了一个相对较“软”的方法将Knowledge合并到回复当中。它主要由3部分组成：utterance GRU、knowledge GRU和fusion unit。 utterance GRU和knowledge GRU这两部分，均使用标准的GRU结构： s_t^y = GRU(y_{t-1}, s_{t-1}, c_t) s_t^k = GRU(k_{i}, s_{t-1}, c_t)然后fusion unit将两个隐藏层合并起来生成解码器$t$时刻的隐藏状态。 s_t = r \odot s_t^y + (1-r)\odot s_t^k其中，$r$控制着隐层状态$s_t$的分布 r = \sigma(W_z[tanh(W_y s_t^y);tanh(W_k s_t^k)])Loss 函数除了KLDivLoss以外，模型中的loss函数还包括NLL Loss和BOW Loss。其中，NLL Loss的作用是最小化生成回复与原始回复之间的差异： \mathcal{L}_{NLL}(\theta)=-\frac{1}{m} \sum_{t=1}^{m} log p_{\theta}(y_t|y]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>dialogue system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《通过对响应原型编辑的回复生成》阅读笔记]]></title>
    <url>%2F%E3%80%8A%E9%80%9A%E8%BF%87%E5%AF%B9%E5%93%8D%E5%BA%94%E5%8E%9F%E5%9E%8B%E7%BC%96%E8%BE%91%E7%9A%84%E5%9B%9E%E5%A4%8D%E7%94%9F%E6%88%90%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文题目：Response Generation by Context-aware Prototype Editing 代码链接：https://github.com/MarkWuNLP/ResponseEdit AAAI 2019 北航 本文的主要目的是为了解决开放域对话生成中，生成句子较短且无意义的问题。模型首先从预定义的索引中检索到响应，然后根据问题原型和当前问题的差异编辑响应原型。主要思想是基于由于响应原型有很好的语法和信息，对响应原型进行轻微地调整就能得到较好的回复这一假设，来进行相关实验和论证。这是一种“检索+生成”的方法，既有检索式聊天机器人回复流畅和信息量大的优势，还有生成式聊天机器人的灵活性和相关性。 模型该模型由原型选择器$\mathcal{S}$和上下文感知编辑器$\mathcal{E}$组成。给定一个新的会话，我们首先使用选择器S来检索语料$\mathcal{D} = {(C_i, R_i)}_{i=1}^{N}$中的$(C_{i}, R_{i}) \in \mathcal{D}$。然后编辑器$\mathcal{E}$计算出$z_i = f(C_i, C)$来编码$C_i$和$C$之间的差异信息。最后，通过概率$p(R|z_i, R_i)$生成回复。 原型选择器(Prototype Selector) 原型选择器使用Lucene框架用于检索，并应用其内置算法计算相似度。 测试阶段 根据上下文$C$与语料中$C^{‘}$的相似性选出（$C^{‘}$ , $R^{‘}$） 训练阶段 由于有真实的response，训练阶段的$C$，$R$对是通过response之间的相似性中选出来的。使用Jaccard相似度，取出范围在0.3-0.7的结果。这样既过滤掉相似度小于0.3的不相关回复，又除去相似度大于0.7的回复，避免直接copy。 Jaccard相似系数 J(A, B) = \frac{|A \cap B|}{|A \cup B|}在这个阶段最终得到多个四元组$(C, R, C^{‘}, R^{‘})$ 上下文感知编辑器（Context-Aware Neural Editor）Edit Vector Generation 该部分的主要功能是根据$C$和$C^{‘}$的差异生成编辑向量（edit vector） 用biGRU对$R^{‘}$进行编码 \overrightarrow{h_j} = f_{GRU}(h_{j-1}, r_{j}^{'}); \overleftarrow{h_j} = f_{GRU}(h_{j+1}, r_{j}^{'})用attention机制计算context之间的diff-vector，其中$I=\{w | w \in C \land w \notin C^{‘}\}$表示插入词集合，$I=\{D | w^{‘} \in C^{‘} \land w^{‘} \notin C \}$表示删除词集合，$\oplus$表示concat操作。 diff_{c} = \sum_{w \in I} \beta_w \Psi(w) \oplus \sum_{w^{'} \in D} \gamma_{w^{'}} \Psi(w^{'}) \beta_w = \frac{exp(e_w)}{\sum_{w \in I}exp(e_w)} e_w = v_{\beta}^{T}tanh(W_{\beta}[\Psi(w)\oplus h_l]) \gamma_{w^{'}} = \frac{exp(e_{w^{'}})}{\sum_{w \in I}exp(e_{w^{'}})} z = tanh(W \cdot diff_{c} + b)Prototype Editing 该部分将编辑向量（edit vector）集成到decoder中进行输出 h_{j}^{\prime}=f_{\mathrm{GRU}}\left(h_{j-1}^{\prime}, r_{j-1} \oplus z_{i}\right) c_{i}=\sum_{j=1}^{t} \alpha_{i, j} h_{j} \begin{aligned} \alpha_{i, j} &=\frac{\exp \left(e_{i, j}\right)}{\sum_{k=1}^{t} \exp \left(e_{i, k}\right)} \\ e_{i, j} &=\mathbf{v}^{\top} \tanh \left(\mathbf{W}_{\alpha}\left[h_{j} \oplus h_{i}^{\prime}\right]\right) \end{aligned} s\left(r_{i}\right)=\operatorname{softmax}\left(\mathbf{W}_{\mathbf{p}}\left[r_{i-1} \oplus h_{i}^{\prime} \oplus c_{i}\right]+\mathbf{b}_{\mathbf{p}}\right)目标函数 \mathcal{L} = - \sum_{i=1}^N\sum_{j=1}^{l}log p(r_{i,j}|z_i, R_{i}^{'}, r_i, k]]></content>
      <categories>
        <category>NLP</category>
        <category>NLG</category>
      </categories>
      <tags>
        <tag>dialogue system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《情感对话生成》阅读笔记]]></title>
    <url>%2F%E3%80%8AEmotional%20Chatting%20Machine%20Emotional%20Conversation%20Generation%20with%20Internal%E3%80%8B%2F</url>
    <content type="text"><![CDATA[论文题目：Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory 论文链接：https://arxiv.org/abs/1704.01074 代码链接：https://github.com/tuxchow/ecm AAAI 2019 Introduction朱小燕、黄民烈老师团队发布的论文「 Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory」首次将情感因素引入了基于深度学习的生成式对话系统，提出了基于记忆网络的情感对话系统 Emotional Chatting Machine (ECM) ，在传统的 Sequence to Sequence 模型的基础上，ECM 使用了静态的情感向量嵌入表示，动态的情感状态记忆网络和情感词外部记忆的机制，使得 ECM 可以根据用户的输入以及指定情感分类输出相应情感的回复语句。 代码链接：https://github.com/tuxchow/ecm 本文用输入作为一种提示和期望的情感的反应，来产生一个回答，采用NLPCC数据集训练一个情感分类器，利用分类器对大规模的对话数据进行自动标注，划分了六类情感，怒、厌恶、快乐、喜欢、悲伤和其他，并用三元数据向量（posts，responses, emotion labels of responses)训练ECM模型。本文设计了一个ECM（Emotional Chatting Machine）框架的，基于记忆网络memory的具有情感生成机制的seq2seq编解码生成模型，模型利用GRU进行编解码。 Model 模型的总体框架如上图所示，用户问题输入为“What a lovely day!”，通过 Encoder 将其编码为隐向量表示 h，然后通过注意力机制，结合 decoder 的状态向量 $s$ 在生成不同的词时，对问题的隐向量表示 h 的不同部分的信息选择性的加强，得到向量 $c$。指定情感类别为“Happiness”，经过索引得到情感类别嵌入向量，初始的情感状态记忆向量和相应的情感词表。decoder 接受经过注意力机制的问题向量 $c$，情感类别嵌入向量和初始的情感状态记忆向量作为输入，通过循环神经网络生成下个词的生成概率 o，之后再经过情感词表对情感词和非情感词的加权，得到最终词的生成概率，通过采样即可得到输出“Haha, so happy today!”。 Encoder-Decoder-Attn框架 编码器将输入序列$X=(x_1,x_2,…, x_n)$转化为隐层表示$h=(h_1,h_2,…, h_n)$: h_t = GRU(h_{t-1}, x_t)解码器将上下文向量（context vector）$c_t$和上一个已解码的词向量$e(y_{t-1})$作为输入，使用另一个GRU来更新隐层状态： s_t = GRU(s_{t-1},[c_t;e(y_{t-1})])$[c_t;e(y_{t-1})]$是两个向量的级联，在我们得到隐层向量$s_t$之后，解码器通过计算得到输出概率分布$o_t$生成新的词： y_t o_t = P(y_t|y_1,y_2,...,y_{t-1},c_t) = softmax(W_os_t)Emotional Category Embedding将情感的类别编码成低维度的向量，然后将情感向量$v_e$、向量$e(y_{t-1})$和上下文向量$c_t$合并到一起作为解码器的输入更新解码器的隐层状态$s_t$： s_{t}=GRU(s_{t-1}, [c_t;e(y_{t-1});v_e])Internal MemoryAffect-lm模型论文中提到简单的情感向量嵌入的方式会影响句子语法的准确性。因此，本文设计了一个内部记忆模块在解码时捕捉情感的动态变化。 本模块受到Annotating and modeling empathy in spoken conversations这篇论文启发，具体结构如上图。在每个时间步都会计算Read Gate $g_t^r$和Write Gate $g_t^r$： g_t^r = sigmoid(W_g^r[e(y_{t-1});s_{t-1};c_t]) g_t^w = sigmoid(W_g^w[e(s_t])$g_t^r$和$g_t^w$是用来对内部记忆模块进行读写操作的。这样每一个时间步,情感状态都会被$g_t^w$消除一部分，这与Key-value memory networks中DELETE操作十分相似。在最后一步时内部的情感信息会被减少至0，表示情感已经被全部表达。 M_{r,t}^I = g_t^r \bigotimes M_{e,t}^I M_{r,t+1}^I = g_t^w \bigotimes M_{e,t}^I s_t = GRU(s_{t-1}, [c_t;e(y_{t-1};M_{e,t}^I)])External Memory针对不同的词汇所表达的感情，模型使用External Memory模块来建模显式的情感表达，给情感词汇和普通词汇赋予不同的生成概率。因此，模型可以选择从情感词汇和普通词汇中选择生成一个词。 \alpha_t = sigmoid(v_u^T s_t) P_g(y_t=w_g) = softmax(W_g^o s_t) P_e(y_t=w_e) = softmax(W_e^o s_t) y_t o_t =P(y_t) = \alpha_t P_e(y_t=w_e) + (1-\alpha_t) P_y(y_t=w_g)损失函数 L(\theta) = - \sum_{t=1}^{m} p_t log(o_t) - \sum_{t=1}^{m} q_tlog(\alpha_t) + ||M_{e,m}^{I}||数据集数据集：NLPCC2017 Shared Task 4 情感分类数据集：NLPCC20132 and NLPCC20143（中文数据）The taxonomy comes from http://tcci.ccf.org.cn/confere-nce/2014/dldoc/evatask1.pdf 大连理工的情感词汇数据集：http://ir.dlut.edu.cn/news/detail/215 主要参考文献 [Ghosh et al. 2017] Ghosh, S.; Chollet, M.; Laksana, E.;Morency, L.; and Scherer, S. 2017. Affect-lm: A neural language model for customizable affective text generation. In ACL, 634–642. [Alam, Danieli, and Riccardi 2017] Alam, F.; Danieli, M.; and Riccardi, G. 2017. Annotating and modeling empathy in spoken conversations. CoRR abs/1705.04839. [Miller et al. 2016] Miller, A. H.; Fisch, A.; Dodge, J.; Karimi, A.; Bordes, A.; and Weston, J. 2016. Keyvalue memory networks for directly reading documents. In EMNLP, 1400–1409.]]></content>
      <categories>
        <category>NLP</category>
        <category>NLG</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
